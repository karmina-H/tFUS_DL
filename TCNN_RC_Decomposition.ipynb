{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dde8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d9dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_decomposition_svd(voxel_grid, rank):\n",
    "    \"\"\"\n",
    "    SVD를 사용하여 3D 복셀 데이터를 C와 R 행렬로 분해합니다.\n",
    "\n",
    "    Args:\n",
    "        voxel_grid (np.ndarray): (101, 101, 101) 형태의 3D 데이터.\n",
    "        rank (int): 분해할 랭크(k). 압축률과 복원 품질을 결정합니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: C 행렬과 R 행렬을 튜플 형태로 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. 3D 복셀 데이터를 2D 행렬로 변환 (Matricization)\n",
    "    # SVD는 2D 행렬에 대해서만 연산이 가능합니다.\n",
    "    # (101, 101, 101) -> (101, 10201)\n",
    "    original_shape = voxel_grid.shape\n",
    "    matrix_a = voxel_grid.reshape(original_shape[0], -1)\n",
    "\n",
    "    # 2. 특잇값 분해(SVD) 수행\n",
    "    # A ≈ U @ S @ Vh\n",
    "    # full_matrices=False 옵션은 연산 속도를 향상시킵니다.\n",
    "    U, S, Vh = np.linalg.svd(matrix_a, full_matrices=False)\n",
    "\n",
    "    # 3. 정의한 rank(k) 만큼 행렬들을 잘라냅니다.\n",
    "    U_k = U[:, :rank]\n",
    "    S_k = np.diag(S[:rank])\n",
    "    Vh_k = Vh[:rank, :]\n",
    "\n",
    "    # 4. C와 R 행렬 정의\n",
    "    # C 행렬: 데이터의 핵심 구조(basis)를 담고 있습니다.\n",
    "    # R 행렬: 이 구조들을 어떻게 조합할지에 대한 계수(coefficients)를 담고 있습니다.\n",
    "    # C @ R 을 통해 원래 행렬 matrix_a를 근사적으로 복원할 수 있습니다.\n",
    "    C = U_k                 # Shape: (101, rank)\n",
    "    R = S_k @ Vh_k          # Shape: (rank, 10201)\n",
    "\n",
    "    return C, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacb710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049eeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RC-Decomposition 이용해서 데이터 압축 및 저장하는 코드\n",
    "# output_d = np.load(\"./Data/output.npy\")\n",
    "\n",
    "# list_of_C = []\n",
    "# list_of_R = []\n",
    "\n",
    "# RANK = 10\n",
    "\n",
    "# for idx, data in enumerate(output_d): # output_d = (1152,101,101,101)\n",
    "#     C, R = rc_decomposition_svd(data, RANK)\n",
    "#     list_of_C.append(C)\n",
    "#     list_of_R.append(R)\n",
    "\n",
    "# output_filename = f'decomposed_data_rank_{RANK}.npz'\n",
    "# np.savez_compressed(\n",
    "#     output_filename,\n",
    "#     C_matrices=np.array(list_of_C),\n",
    "#     R_matrices=np.array(list_of_R)\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5648df",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b037537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 10, 10201)\n",
      "(1125, 101, 10)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "\n",
    "output_d = np.load(\"./Data/decomposed_data_rank_10.npz\")\n",
    "\n",
    "# R을 예측하는 것\n",
    "output_R = output_d['R_matrices']\n",
    "output_C = output_d['C_matrices']\n",
    "\n",
    "print(output_R.shape)\n",
    "print(output_C.shape)\n",
    "\n",
    "tensor_data_out = torch.from_numpy(output_R).float()\n",
    "\n",
    "input_d = np.load(\"./Data/input.npy\")\n",
    "tensor_data_in = torch.from_numpy(input_d).float()\n",
    "\n",
    "dataset = TensorDataset(tensor_data_in, tensor_data_out)\n",
    "\n",
    "val_split = 0.1\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(dataset_size * val_split)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=3)\n",
    "\n",
    "vali_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283335d",
   "metadata": {},
   "source": [
    "### 모델구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8478172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=6, output_channels=10, output_features=10201):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.output_features_per_channel = output_features\n",
    "\n",
    "        # 목표 출력 크기\n",
    "        self.final_output_size = output_channels * output_features\n",
    "\n",
    "        # 1. 초기 MLP: 6개의 입력 값을 1024개로 확장\n",
    "        self.initial_mlp = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "        )\n",
    "\n",
    "        # (batch, 1024) -> reshape to (batch, 256, 2, 2)\n",
    "        # 2x2 크기에서 시작하여 32x32까지 점진적으로 확장\n",
    "        self.upsample_blocks = nn.Sequential(\n",
    "            # 입력: (batch, 256, 2, 2)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> (batch, 128, 4, 4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # -> (batch, 64, 8, 8)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # -> (batch, 32, 16, 16)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # -> (batch, 16, 32, 32)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # 3. 최종 MLP: 업샘플링된 피처를 최종 출력 크기로 매핑\n",
    "        # (16 * 32 * 32 = 16384) -> (10 * 10201 = 102010)\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(16 * 32 * 32, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4096, self.final_output_size),\n",
    "            # 마지막 활성화 함수는 문제에 따라 tanh, sigmoid 등을 추가할 수 있습니다.\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 초기 MLP 통과\n",
    "        out = self.initial_mlp(x) # (batch, 1024)\n",
    "\n",
    "        # 2. ConvTranspose2d를 위한 형태로 변경\n",
    "        # (batch, 256, 2, 2)\n",
    "        out = out.view(out.size(0), 256, 2, 2)\n",
    "\n",
    "        # 3. 업샘플링 블록 통과\n",
    "        out = self.upsample_blocks(out) # (batch, 16, 32, 32)\n",
    "\n",
    "        # 4. 최종 MLP를 위한 형태로 변경\n",
    "        out = out.view(out.size(0), -1) # (batch, 16*32*32)\n",
    "\n",
    "        # 5. 최종 MLP 통과\n",
    "        out = self.final_mlp(out) # (batch, 10 * 10201)\n",
    "\n",
    "        # 6. 최종 출력 형태로 변경\n",
    "        out = out.view(out.size(0), self.output_channels, self.output_features_per_channel) # (batch, 10, 10201)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b07c0",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a1cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f314f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화용 list\n",
    "train_loss_list = []\n",
    "vali_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254dd7cf",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7cdceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3000] Train Loss: 0.004355 | Val Loss: 0.000914\n",
      "[Epoch 2/3000] Train Loss: 0.000760 | Val Loss: 0.000752\n",
      "[Epoch 3/3000] Train Loss: 0.000716 | Val Loss: 0.000718\n",
      "[Epoch 4/3000] Train Loss: 0.000711 | Val Loss: 0.000738\n",
      "[Epoch 5/3000] Train Loss: 0.000709 | Val Loss: 0.000732\n",
      "[Epoch 6/3000] Train Loss: 0.000706 | Val Loss: 0.000736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     loss.backward()\n\u001b[32m     18\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     epoch_loss_train += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m avg_train_loss = epoch_loss_train / \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[32m     24\u001b[39m train_loss_list.append(avg_train_loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "\n",
    "plt.ion()  # interactive mode ON\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss_train = 0.0\n",
    "    epoch_loss_vali = 0.0\n",
    "\n",
    "    for batch_in, batch_out in train_dataloader:\n",
    "        batch_in, batch_out = batch_in.to(device), batch_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_in)\n",
    "        loss = criterion(y_pred, batch_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "\n",
    "    avg_train_loss = epoch_loss_train / len(train_dataloader)\n",
    "            \n",
    "    train_loss_list.append(avg_train_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in vali_dataloader:\n",
    "            batch_in, batch_out = batch_in.to(device), batch_out.to(device)\n",
    "            y_pred = model(batch_in)\n",
    "            val_loss = criterion(y_pred, batch_out)\n",
    "\n",
    "            epoch_loss_vali += val_loss.item()\n",
    "\n",
    "\n",
    "    avg_vali_loss = epoch_loss_vali / len(vali_dataloader)\n",
    "    vali_loss_list.append(avg_vali_loss)\n",
    "\n",
    "    # --- 실시간 그래프 업데이트 ---\n",
    "    ax.clear()\n",
    "    epochs_ran = range(len(train_loss_list))\n",
    "    ax.plot(epochs_ran, train_loss_list, label=\"Training Loss\", color=\"blue\")\n",
    "    ax.plot(epochs_ran, vali_loss_list, label=\"Validation Loss\", color=\"red\") # validation -> Validation (오타 수정)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE Loss\")\n",
    "    ax.set_title(f\"Simple_MLP Training Loss [Epoch {epoch+1}/{epochs}]\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    plt.pause(0.01) # <--- [수정 3: 그래프가 확실히 보이도록 추가]\n",
    "\n",
    "    # 훈련 손실과 검증 손실을 함께 출력하는 것이 좋습니다.\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_vali_loss:.6f}\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# ----- 모델 저장 -----\n",
    "torch.save(model.state_dict(), \"Simple_MLP.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
