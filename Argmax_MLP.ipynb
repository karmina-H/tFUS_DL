{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16d5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f57c5c",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdcf104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax_output len : 1125\n",
      "argmax_output len : 1125\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "output_d = np.load(\"./Data/output.npy\")\n",
    "argmax_output = np.empty((len(output_d),4))\n",
    "\n",
    "print(f'argmax_output len : {len(argmax_output)}')\n",
    "for idx, data in enumerate(output_d):\n",
    "    max_v = np.max(data)\n",
    "    flatten_index = np.argmax(data)\n",
    "    z,y,x = np.unravel_index(flatten_index, data.shape)\n",
    "    argmax_output[idx] = [z,y,x, max_v]\n",
    "    \n",
    "print(f'argmax_output len : {len(argmax_output)}')\n",
    "    \n",
    "tensor_data_out = torch.from_numpy(argmax_output).float()\n",
    "\n",
    "input_d = np.load(\"./Data/input.npy\")\n",
    "tensor_data_in = torch.from_numpy(input_d).float()\n",
    "\n",
    "\n",
    "dataset = TensorDataset(tensor_data_in, tensor_data_out)\n",
    "\n",
    "val_split = 0.1\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(dataset_size * val_split)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False, num_workers=3)\n",
    "\n",
    "vali_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True, drop_last=False, num_workers=3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ec9f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(output_d.dtype)\n",
    "print(input_d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff8767",
   "metadata": {},
   "source": [
    "### Model Structure Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de122910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module을 상속받는 MLP 클래스 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_sizes[0], dim_sizes[1]),\n",
    "            nn.GELU(),\n",
    "             nn.BatchNorm1d(dim_sizes[1]),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(dim_sizes[1], dim_sizes[2]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(dim_sizes[2]),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(dim_sizes[2], dim_sizes[3]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(dim_sizes[3]),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(dim_sizes[3], dim_sizes[4]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(dim_sizes[4]),\n",
    "\n",
    "            nn.Linear(dim_sizes[4], dim_sizes[5]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(dim_sizes[5]),\n",
    "\n",
    "            nn.Linear(dim_sizes[5], dim_sizes[6])\n",
    "        )\n",
    "    \n",
    "    # 결과 = 패치위치(3차원) + 초음파강도(raw value)(1차원) = 4차원벡터\n",
    "    def forward(self, x):\n",
    "        result = self.mlp(x)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a669054",
   "metadata": {},
   "source": [
    "### Loss, HpyerParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71cc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 모델/손실/옵티마이저 -----\n",
    "dim_sizes = [6,24,48,96,48,24,4] # 처음 input dim부터 인덱스 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(dim_sizes).to(device) # -> 여기 MLP로 바꿔야함\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4ba88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화용 list\n",
    "train_loss_list = []\n",
    "vali_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cd1fb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cdfa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2000] Train Loss: 2038.123901 | Val Loss: 2106.610107\n",
      "[Epoch 2/2000] Train Loss: 2038.672058 | Val Loss: 2105.583740\n",
      "[Epoch 3/2000] Train Loss: 2037.091522 | Val Loss: 2104.193848\n",
      "[Epoch 4/2000] Train Loss: 2034.637833 | Val Loss: 2104.088623\n",
      "[Epoch 5/2000] Train Loss: 2032.999954 | Val Loss: 2104.312012\n",
      "[Epoch 6/2000] Train Loss: 2034.247742 | Val Loss: 2104.000244\n",
      "[Epoch 7/2000] Train Loss: 2031.765472 | Val Loss: 2102.039795\n",
      "[Epoch 8/2000] Train Loss: 2029.834976 | Val Loss: 2100.696777\n",
      "[Epoch 9/2000] Train Loss: 2026.074280 | Val Loss: 2098.838867\n",
      "[Epoch 10/2000] Train Loss: 2027.523148 | Val Loss: 2097.524902\n",
      "[Epoch 11/2000] Train Loss: 2025.225739 | Val Loss: 2095.620117\n",
      "[Epoch 12/2000] Train Loss: 2024.535034 | Val Loss: 2093.798828\n",
      "[Epoch 13/2000] Train Loss: 2024.730530 | Val Loss: 2091.663818\n",
      "[Epoch 14/2000] Train Loss: 2023.403748 | Val Loss: 2091.687500\n",
      "[Epoch 15/2000] Train Loss: 2021.834076 | Val Loss: 2089.769775\n",
      "[Epoch 16/2000] Train Loss: 2019.719498 | Val Loss: 2087.913574\n",
      "[Epoch 17/2000] Train Loss: 2021.030151 | Val Loss: 2086.818359\n",
      "[Epoch 18/2000] Train Loss: 2019.803986 | Val Loss: 2084.241943\n",
      "[Epoch 19/2000] Train Loss: 2019.578781 | Val Loss: 2083.495850\n",
      "[Epoch 20/2000] Train Loss: 2019.466751 | Val Loss: 2083.627686\n",
      "[Epoch 21/2000] Train Loss: 2015.809799 | Val Loss: 2081.308350\n",
      "[Epoch 22/2000] Train Loss: 2015.992981 | Val Loss: 2079.579102\n",
      "[Epoch 23/2000] Train Loss: 2015.047134 | Val Loss: 2078.201172\n",
      "[Epoch 24/2000] Train Loss: 2015.785675 | Val Loss: 2076.554199\n",
      "[Epoch 25/2000] Train Loss: 2014.449081 | Val Loss: 2075.831299\n",
      "[Epoch 26/2000] Train Loss: 2014.702271 | Val Loss: 2074.918457\n",
      "[Epoch 27/2000] Train Loss: 2013.722076 | Val Loss: 2074.027344\n",
      "[Epoch 28/2000] Train Loss: 2011.491760 | Val Loss: 2071.871582\n",
      "[Epoch 29/2000] Train Loss: 2010.488876 | Val Loss: 2070.831543\n",
      "[Epoch 30/2000] Train Loss: 2011.072800 | Val Loss: 2071.398926\n",
      "[Epoch 31/2000] Train Loss: 2009.707016 | Val Loss: 2069.093506\n",
      "[Epoch 32/2000] Train Loss: 2009.606812 | Val Loss: 2068.752686\n",
      "[Epoch 33/2000] Train Loss: 2008.860336 | Val Loss: 2066.619385\n",
      "[Epoch 34/2000] Train Loss: 2008.345673 | Val Loss: 2066.459961\n",
      "[Epoch 35/2000] Train Loss: 2008.068237 | Val Loss: 2066.118896\n",
      "[Epoch 36/2000] Train Loss: 2006.915222 | Val Loss: 2064.882568\n",
      "[Epoch 37/2000] Train Loss: 2005.995483 | Val Loss: 2063.521484\n",
      "[Epoch 38/2000] Train Loss: 2006.500946 | Val Loss: 2062.237793\n",
      "[Epoch 39/2000] Train Loss: 2005.788498 | Val Loss: 2063.889160\n",
      "[Epoch 40/2000] Train Loss: 2004.886002 | Val Loss: 2062.193359\n",
      "[Epoch 41/2000] Train Loss: 2003.250198 | Val Loss: 2060.237061\n",
      "[Epoch 42/2000] Train Loss: 2004.105179 | Val Loss: 2059.895508\n",
      "[Epoch 43/2000] Train Loss: 2004.090927 | Val Loss: 2060.016357\n",
      "[Epoch 44/2000] Train Loss: 2002.454147 | Val Loss: 2060.506836\n",
      "[Epoch 45/2000] Train Loss: 2003.345596 | Val Loss: 2058.380859\n",
      "[Epoch 46/2000] Train Loss: 2002.442795 | Val Loss: 2059.591064\n",
      "[Epoch 47/2000] Train Loss: 2000.831070 | Val Loss: 2059.044922\n",
      "[Epoch 48/2000] Train Loss: 1999.969193 | Val Loss: 2056.660156\n",
      "[Epoch 49/2000] Train Loss: 1999.438843 | Val Loss: 2056.454834\n",
      "[Epoch 50/2000] Train Loss: 1999.493713 | Val Loss: 2055.415283\n",
      "[Epoch 51/2000] Train Loss: 1997.679779 | Val Loss: 2053.119873\n",
      "[Epoch 52/2000] Train Loss: 1997.909821 | Val Loss: 2053.051758\n",
      "[Epoch 53/2000] Train Loss: 1997.962799 | Val Loss: 2053.603760\n",
      "[Epoch 54/2000] Train Loss: 1997.792938 | Val Loss: 2052.743408\n",
      "[Epoch 55/2000] Train Loss: 1996.294891 | Val Loss: 2051.093750\n",
      "[Epoch 56/2000] Train Loss: 1995.874435 | Val Loss: 2049.027832\n",
      "[Epoch 57/2000] Train Loss: 1995.133362 | Val Loss: 2049.235840\n",
      "[Epoch 58/2000] Train Loss: 1994.485535 | Val Loss: 2049.426514\n",
      "[Epoch 59/2000] Train Loss: 1993.982452 | Val Loss: 2049.051514\n",
      "[Epoch 60/2000] Train Loss: 1993.340057 | Val Loss: 2047.481934\n",
      "[Epoch 61/2000] Train Loss: 1994.102905 | Val Loss: 2046.748413\n",
      "[Epoch 62/2000] Train Loss: 1992.400314 | Val Loss: 2047.113037\n",
      "[Epoch 63/2000] Train Loss: 1991.927597 | Val Loss: 2046.690186\n",
      "[Epoch 64/2000] Train Loss: 1991.611633 | Val Loss: 2044.704102\n",
      "[Epoch 65/2000] Train Loss: 1990.497726 | Val Loss: 2044.509399\n",
      "[Epoch 66/2000] Train Loss: 1991.284744 | Val Loss: 2043.820801\n",
      "[Epoch 67/2000] Train Loss: 1989.618774 | Val Loss: 2042.202515\n",
      "[Epoch 68/2000] Train Loss: 1988.471924 | Val Loss: 2039.520630\n",
      "[Epoch 69/2000] Train Loss: 1989.254593 | Val Loss: 2041.332153\n",
      "[Epoch 70/2000] Train Loss: 1987.750015 | Val Loss: 2038.496460\n",
      "[Epoch 71/2000] Train Loss: 1987.553436 | Val Loss: 2039.549438\n",
      "[Epoch 72/2000] Train Loss: 1988.826096 | Val Loss: 2038.049805\n",
      "[Epoch 73/2000] Train Loss: 1985.818863 | Val Loss: 2037.840210\n",
      "[Epoch 74/2000] Train Loss: 1985.805252 | Val Loss: 2038.911499\n",
      "[Epoch 75/2000] Train Loss: 1985.399689 | Val Loss: 2037.599121\n",
      "[Epoch 76/2000] Train Loss: 1984.020966 | Val Loss: 2035.595459\n",
      "[Epoch 77/2000] Train Loss: 1984.265961 | Val Loss: 2036.848633\n",
      "[Epoch 78/2000] Train Loss: 1982.355667 | Val Loss: 2034.770752\n",
      "[Epoch 79/2000] Train Loss: 1982.684982 | Val Loss: 2033.127686\n",
      "[Epoch 80/2000] Train Loss: 1982.044983 | Val Loss: 2033.715942\n",
      "[Epoch 81/2000] Train Loss: 1982.151672 | Val Loss: 2033.714844\n",
      "[Epoch 82/2000] Train Loss: 1980.209793 | Val Loss: 2030.673218\n",
      "[Epoch 83/2000] Train Loss: 1981.201660 | Val Loss: 2029.410278\n",
      "[Epoch 84/2000] Train Loss: 1978.956955 | Val Loss: 2028.354126\n",
      "[Epoch 85/2000] Train Loss: 1978.937698 | Val Loss: 2028.402466\n",
      "[Epoch 86/2000] Train Loss: 1978.831940 | Val Loss: 2028.988037\n",
      "[Epoch 87/2000] Train Loss: 1976.250519 | Val Loss: 2026.841797\n",
      "[Epoch 88/2000] Train Loss: 1976.337143 | Val Loss: 2025.962646\n",
      "[Epoch 89/2000] Train Loss: 1976.268784 | Val Loss: 2027.806152\n",
      "[Epoch 90/2000] Train Loss: 1976.726288 | Val Loss: 2026.643677\n",
      "[Epoch 91/2000] Train Loss: 1975.741333 | Val Loss: 2025.116577\n",
      "[Epoch 92/2000] Train Loss: 1975.694397 | Val Loss: 2023.545166\n",
      "[Epoch 93/2000] Train Loss: 1973.671585 | Val Loss: 2022.286133\n",
      "[Epoch 94/2000] Train Loss: 1973.035690 | Val Loss: 2022.347168\n",
      "[Epoch 95/2000] Train Loss: 1972.419006 | Val Loss: 2020.713867\n",
      "[Epoch 96/2000] Train Loss: 1971.835678 | Val Loss: 2020.920288\n",
      "[Epoch 97/2000] Train Loss: 1970.626740 | Val Loss: 2019.274902\n",
      "[Epoch 98/2000] Train Loss: 1970.456284 | Val Loss: 2020.076904\n",
      "[Epoch 99/2000] Train Loss: 1970.059296 | Val Loss: 2018.362793\n",
      "[Epoch 100/2000] Train Loss: 1967.419861 | Val Loss: 2016.515747\n",
      "[Epoch 101/2000] Train Loss: 1967.795578 | Val Loss: 2014.846924\n",
      "[Epoch 102/2000] Train Loss: 1967.791855 | Val Loss: 2014.100708\n",
      "[Epoch 103/2000] Train Loss: 1966.927795 | Val Loss: 2013.955078\n",
      "[Epoch 104/2000] Train Loss: 1966.104126 | Val Loss: 2013.299072\n",
      "[Epoch 105/2000] Train Loss: 1965.076508 | Val Loss: 2012.187988\n",
      "[Epoch 106/2000] Train Loss: 1965.887070 | Val Loss: 2011.332275\n",
      "[Epoch 107/2000] Train Loss: 1964.497910 | Val Loss: 2011.657471\n",
      "[Epoch 108/2000] Train Loss: 1962.748718 | Val Loss: 2010.011963\n",
      "[Epoch 109/2000] Train Loss: 1964.363327 | Val Loss: 2010.662720\n",
      "[Epoch 110/2000] Train Loss: 1960.997559 | Val Loss: 2010.531860\n",
      "[Epoch 111/2000] Train Loss: 1960.712112 | Val Loss: 2007.764771\n",
      "[Epoch 112/2000] Train Loss: 1960.787277 | Val Loss: 2006.813232\n",
      "[Epoch 113/2000] Train Loss: 1960.553894 | Val Loss: 2006.538574\n",
      "[Epoch 114/2000] Train Loss: 1959.711151 | Val Loss: 2008.111694\n",
      "[Epoch 115/2000] Train Loss: 1957.658997 | Val Loss: 2003.208252\n",
      "[Epoch 116/2000] Train Loss: 1957.472076 | Val Loss: 2002.889893\n",
      "[Epoch 117/2000] Train Loss: 1957.724838 | Val Loss: 2004.955688\n",
      "[Epoch 118/2000] Train Loss: 1957.238235 | Val Loss: 2003.552124\n",
      "[Epoch 119/2000] Train Loss: 1955.461090 | Val Loss: 2001.586914\n",
      "[Epoch 120/2000] Train Loss: 1954.576218 | Val Loss: 2000.819336\n",
      "[Epoch 121/2000] Train Loss: 1954.911209 | Val Loss: 2001.193726\n",
      "[Epoch 122/2000] Train Loss: 1953.637482 | Val Loss: 1998.766235\n",
      "[Epoch 123/2000] Train Loss: 1951.959503 | Val Loss: 1997.676514\n",
      "[Epoch 124/2000] Train Loss: 1950.676041 | Val Loss: 1998.977539\n",
      "[Epoch 125/2000] Train Loss: 1949.701767 | Val Loss: 1997.089111\n",
      "[Epoch 126/2000] Train Loss: 1949.879471 | Val Loss: 1996.598022\n",
      "[Epoch 127/2000] Train Loss: 1948.552322 | Val Loss: 1992.774048\n",
      "[Epoch 128/2000] Train Loss: 1947.949356 | Val Loss: 1992.090332\n",
      "[Epoch 129/2000] Train Loss: 1948.132431 | Val Loss: 1991.203247\n",
      "[Epoch 130/2000] Train Loss: 1947.110580 | Val Loss: 1991.505981\n",
      "[Epoch 131/2000] Train Loss: 1946.228287 | Val Loss: 1990.186157\n",
      "[Epoch 132/2000] Train Loss: 1944.788651 | Val Loss: 1987.311768\n",
      "[Epoch 133/2000] Train Loss: 1944.538177 | Val Loss: 1987.206299\n",
      "[Epoch 134/2000] Train Loss: 1943.367538 | Val Loss: 1983.540039\n",
      "[Epoch 135/2000] Train Loss: 1942.381302 | Val Loss: 1986.381836\n",
      "[Epoch 136/2000] Train Loss: 1942.447845 | Val Loss: 1988.701904\n",
      "[Epoch 137/2000] Train Loss: 1941.875290 | Val Loss: 1986.050171\n",
      "[Epoch 138/2000] Train Loss: 1940.820877 | Val Loss: 1984.701294\n",
      "[Epoch 139/2000] Train Loss: 1939.073486 | Val Loss: 1985.171143\n",
      "[Epoch 140/2000] Train Loss: 1938.352310 | Val Loss: 1982.736450\n",
      "[Epoch 141/2000] Train Loss: 1936.550247 | Val Loss: 1982.415527\n",
      "[Epoch 142/2000] Train Loss: 1937.153534 | Val Loss: 1980.825562\n",
      "[Epoch 143/2000] Train Loss: 1935.182465 | Val Loss: 1980.746460\n",
      "[Epoch 144/2000] Train Loss: 1936.065216 | Val Loss: 1978.215820\n",
      "[Epoch 145/2000] Train Loss: 1936.902344 | Val Loss: 1981.033813\n",
      "[Epoch 146/2000] Train Loss: 1933.765930 | Val Loss: 1980.396240\n",
      "[Epoch 147/2000] Train Loss: 1930.880707 | Val Loss: 1978.567627\n",
      "[Epoch 148/2000] Train Loss: 1932.317688 | Val Loss: 1976.009033\n",
      "[Epoch 149/2000] Train Loss: 1930.183472 | Val Loss: 1972.894165\n",
      "[Epoch 150/2000] Train Loss: 1930.459183 | Val Loss: 1971.796509\n",
      "[Epoch 151/2000] Train Loss: 1929.439819 | Val Loss: 1974.590454\n",
      "[Epoch 152/2000] Train Loss: 1928.837448 | Val Loss: 1972.781006\n",
      "[Epoch 153/2000] Train Loss: 1926.362946 | Val Loss: 1968.096069\n",
      "[Epoch 154/2000] Train Loss: 1926.341873 | Val Loss: 1967.802002\n",
      "[Epoch 155/2000] Train Loss: 1925.718643 | Val Loss: 1967.085449\n",
      "[Epoch 156/2000] Train Loss: 1926.401901 | Val Loss: 1960.982056\n",
      "[Epoch 157/2000] Train Loss: 1925.057693 | Val Loss: 1961.948486\n",
      "[Epoch 158/2000] Train Loss: 1921.935822 | Val Loss: 1962.145874\n",
      "[Epoch 159/2000] Train Loss: 1922.761841 | Val Loss: 1965.443848\n",
      "[Epoch 160/2000] Train Loss: 1921.716721 | Val Loss: 1967.997192\n",
      "[Epoch 161/2000] Train Loss: 1919.874908 | Val Loss: 1963.013794\n",
      "[Epoch 162/2000] Train Loss: 1918.916000 | Val Loss: 1958.233521\n",
      "[Epoch 163/2000] Train Loss: 1919.881729 | Val Loss: 1957.761841\n",
      "[Epoch 164/2000] Train Loss: 1916.704361 | Val Loss: 1956.482056\n",
      "[Epoch 165/2000] Train Loss: 1916.732346 | Val Loss: 1957.116943\n",
      "[Epoch 166/2000] Train Loss: 1915.203995 | Val Loss: 1956.449341\n",
      "[Epoch 167/2000] Train Loss: 1915.290970 | Val Loss: 1954.943481\n",
      "[Epoch 168/2000] Train Loss: 1914.290558 | Val Loss: 1954.023560\n",
      "[Epoch 169/2000] Train Loss: 1913.068207 | Val Loss: 1949.913574\n",
      "[Epoch 170/2000] Train Loss: 1912.384293 | Val Loss: 1950.897461\n",
      "[Epoch 171/2000] Train Loss: 1910.388672 | Val Loss: 1950.297485\n",
      "[Epoch 172/2000] Train Loss: 1909.215363 | Val Loss: 1949.847168\n",
      "[Epoch 173/2000] Train Loss: 1908.914062 | Val Loss: 1947.021606\n",
      "[Epoch 174/2000] Train Loss: 1909.976501 | Val Loss: 1946.889404\n",
      "[Epoch 175/2000] Train Loss: 1907.782913 | Val Loss: 1948.875366\n",
      "[Epoch 176/2000] Train Loss: 1907.084747 | Val Loss: 1949.853149\n",
      "[Epoch 177/2000] Train Loss: 1905.995453 | Val Loss: 1947.625000\n",
      "[Epoch 178/2000] Train Loss: 1905.662781 | Val Loss: 1947.735840\n",
      "[Epoch 179/2000] Train Loss: 1903.866562 | Val Loss: 1945.245361\n",
      "[Epoch 180/2000] Train Loss: 1902.757462 | Val Loss: 1940.024536\n",
      "[Epoch 181/2000] Train Loss: 1903.370468 | Val Loss: 1942.043945\n",
      "[Epoch 182/2000] Train Loss: 1899.099411 | Val Loss: 1937.943726\n",
      "[Epoch 183/2000] Train Loss: 1899.986115 | Val Loss: 1938.387695\n",
      "[Epoch 184/2000] Train Loss: 1898.387283 | Val Loss: 1938.031372\n",
      "[Epoch 185/2000] Train Loss: 1898.160782 | Val Loss: 1933.475342\n",
      "[Epoch 186/2000] Train Loss: 1896.591446 | Val Loss: 1933.088867\n",
      "[Epoch 187/2000] Train Loss: 1896.573349 | Val Loss: 1936.613892\n",
      "[Epoch 188/2000] Train Loss: 1895.593719 | Val Loss: 1930.218262\n",
      "[Epoch 189/2000] Train Loss: 1894.219681 | Val Loss: 1926.741943\n",
      "[Epoch 190/2000] Train Loss: 1895.784653 | Val Loss: 1927.656006\n",
      "[Epoch 191/2000] Train Loss: 1892.346756 | Val Loss: 1929.590454\n",
      "[Epoch 192/2000] Train Loss: 1889.012619 | Val Loss: 1925.214966\n",
      "[Epoch 193/2000] Train Loss: 1889.684662 | Val Loss: 1925.230225\n",
      "[Epoch 194/2000] Train Loss: 1888.042664 | Val Loss: 1922.973267\n",
      "[Epoch 195/2000] Train Loss: 1887.898300 | Val Loss: 1920.413330\n",
      "[Epoch 196/2000] Train Loss: 1885.750443 | Val Loss: 1918.262939\n",
      "[Epoch 197/2000] Train Loss: 1884.053619 | Val Loss: 1916.626465\n",
      "[Epoch 198/2000] Train Loss: 1884.915176 | Val Loss: 1919.364502\n",
      "[Epoch 199/2000] Train Loss: 1881.509338 | Val Loss: 1921.270996\n",
      "[Epoch 200/2000] Train Loss: 1882.909821 | Val Loss: 1918.973022\n",
      "[Epoch 201/2000] Train Loss: 1881.428711 | Val Loss: 1917.840088\n",
      "[Epoch 202/2000] Train Loss: 1882.036041 | Val Loss: 1917.496338\n",
      "[Epoch 203/2000] Train Loss: 1878.885910 | Val Loss: 1915.438721\n",
      "[Epoch 204/2000] Train Loss: 1878.801804 | Val Loss: 1913.458008\n",
      "[Epoch 205/2000] Train Loss: 1876.898682 | Val Loss: 1913.615601\n",
      "[Epoch 206/2000] Train Loss: 1876.771301 | Val Loss: 1911.627441\n",
      "[Epoch 207/2000] Train Loss: 1875.756500 | Val Loss: 1910.213867\n",
      "[Epoch 208/2000] Train Loss: 1873.643951 | Val Loss: 1912.718506\n",
      "[Epoch 209/2000] Train Loss: 1874.592621 | Val Loss: 1909.646240\n",
      "[Epoch 210/2000] Train Loss: 1872.841797 | Val Loss: 1910.675903\n",
      "[Epoch 211/2000] Train Loss: 1873.368362 | Val Loss: 1909.840210\n",
      "[Epoch 212/2000] Train Loss: 1872.265396 | Val Loss: 1909.750488\n",
      "[Epoch 213/2000] Train Loss: 1869.458588 | Val Loss: 1900.506348\n",
      "[Epoch 214/2000] Train Loss: 1868.419205 | Val Loss: 1902.045288\n",
      "[Epoch 215/2000] Train Loss: 1866.614532 | Val Loss: 1900.039429\n",
      "[Epoch 216/2000] Train Loss: 1867.065094 | Val Loss: 1902.650024\n",
      "[Epoch 217/2000] Train Loss: 1865.868958 | Val Loss: 1898.739014\n",
      "[Epoch 218/2000] Train Loss: 1863.678391 | Val Loss: 1896.688843\n",
      "[Epoch 219/2000] Train Loss: 1863.243896 | Val Loss: 1895.144897\n",
      "[Epoch 220/2000] Train Loss: 1862.287994 | Val Loss: 1893.368408\n",
      "[Epoch 221/2000] Train Loss: 1861.377777 | Val Loss: 1890.330688\n",
      "[Epoch 222/2000] Train Loss: 1859.881302 | Val Loss: 1890.358154\n",
      "[Epoch 223/2000] Train Loss: 1860.121704 | Val Loss: 1891.885132\n",
      "[Epoch 224/2000] Train Loss: 1857.348053 | Val Loss: 1891.250122\n",
      "[Epoch 225/2000] Train Loss: 1856.290710 | Val Loss: 1890.904541\n",
      "[Epoch 226/2000] Train Loss: 1856.087433 | Val Loss: 1889.025879\n",
      "[Epoch 227/2000] Train Loss: 1854.642715 | Val Loss: 1890.674438\n",
      "[Epoch 228/2000] Train Loss: 1853.380722 | Val Loss: 1885.767944\n",
      "[Epoch 229/2000] Train Loss: 1851.249954 | Val Loss: 1885.646240\n",
      "[Epoch 230/2000] Train Loss: 1851.600693 | Val Loss: 1883.615601\n",
      "[Epoch 231/2000] Train Loss: 1851.498520 | Val Loss: 1884.203247\n",
      "[Epoch 232/2000] Train Loss: 1849.074387 | Val Loss: 1882.555054\n",
      "[Epoch 233/2000] Train Loss: 1849.441040 | Val Loss: 1882.179932\n",
      "[Epoch 234/2000] Train Loss: 1845.836594 | Val Loss: 1880.225586\n",
      "[Epoch 235/2000] Train Loss: 1845.480179 | Val Loss: 1877.067383\n",
      "[Epoch 236/2000] Train Loss: 1844.304565 | Val Loss: 1880.680298\n",
      "[Epoch 237/2000] Train Loss: 1844.060562 | Val Loss: 1874.128418\n",
      "[Epoch 238/2000] Train Loss: 1842.574463 | Val Loss: 1875.462158\n",
      "[Epoch 239/2000] Train Loss: 1841.497040 | Val Loss: 1874.199463\n",
      "[Epoch 240/2000] Train Loss: 1839.712280 | Val Loss: 1871.266724\n",
      "[Epoch 241/2000] Train Loss: 1840.648407 | Val Loss: 1873.875977\n",
      "[Epoch 242/2000] Train Loss: 1837.454681 | Val Loss: 1871.380127\n",
      "[Epoch 243/2000] Train Loss: 1836.411850 | Val Loss: 1866.651123\n",
      "[Epoch 244/2000] Train Loss: 1835.724518 | Val Loss: 1863.366211\n",
      "[Epoch 245/2000] Train Loss: 1835.755478 | Val Loss: 1865.712158\n",
      "[Epoch 246/2000] Train Loss: 1832.981522 | Val Loss: 1863.549316\n",
      "[Epoch 247/2000] Train Loss: 1832.155075 | Val Loss: 1860.437012\n",
      "[Epoch 248/2000] Train Loss: 1830.743469 | Val Loss: 1861.549072\n",
      "[Epoch 249/2000] Train Loss: 1829.647095 | Val Loss: 1864.381226\n",
      "[Epoch 250/2000] Train Loss: 1827.959915 | Val Loss: 1863.890625\n",
      "[Epoch 251/2000] Train Loss: 1826.835388 | Val Loss: 1861.929443\n",
      "[Epoch 252/2000] Train Loss: 1826.728302 | Val Loss: 1860.969238\n",
      "[Epoch 253/2000] Train Loss: 1825.715073 | Val Loss: 1859.428345\n",
      "[Epoch 254/2000] Train Loss: 1824.453918 | Val Loss: 1858.417480\n",
      "[Epoch 255/2000] Train Loss: 1824.592484 | Val Loss: 1853.639038\n",
      "[Epoch 256/2000] Train Loss: 1821.785492 | Val Loss: 1852.998291\n",
      "[Epoch 257/2000] Train Loss: 1821.191559 | Val Loss: 1856.716919\n",
      "[Epoch 258/2000] Train Loss: 1820.417374 | Val Loss: 1852.444824\n",
      "[Epoch 259/2000] Train Loss: 1818.657913 | Val Loss: 1857.711548\n",
      "[Epoch 260/2000] Train Loss: 1816.837952 | Val Loss: 1848.642700\n",
      "[Epoch 261/2000] Train Loss: 1816.834274 | Val Loss: 1847.783569\n",
      "[Epoch 262/2000] Train Loss: 1813.161987 | Val Loss: 1842.215820\n",
      "[Epoch 263/2000] Train Loss: 1814.810898 | Val Loss: 1841.551392\n",
      "[Epoch 264/2000] Train Loss: 1813.595490 | Val Loss: 1845.754272\n",
      "[Epoch 265/2000] Train Loss: 1811.738937 | Val Loss: 1847.037720\n",
      "[Epoch 266/2000] Train Loss: 1812.123138 | Val Loss: 1841.896484\n",
      "[Epoch 267/2000] Train Loss: 1809.636642 | Val Loss: 1840.474121\n",
      "[Epoch 268/2000] Train Loss: 1808.231155 | Val Loss: 1837.280273\n",
      "[Epoch 269/2000] Train Loss: 1806.022308 | Val Loss: 1834.152100\n",
      "[Epoch 270/2000] Train Loss: 1804.272598 | Val Loss: 1832.915771\n",
      "[Epoch 271/2000] Train Loss: 1804.425064 | Val Loss: 1828.346069\n",
      "[Epoch 272/2000] Train Loss: 1803.297287 | Val Loss: 1832.689819\n",
      "[Epoch 273/2000] Train Loss: 1801.022507 | Val Loss: 1830.614746\n",
      "[Epoch 274/2000] Train Loss: 1800.240509 | Val Loss: 1829.897461\n",
      "[Epoch 275/2000] Train Loss: 1800.796631 | Val Loss: 1820.303955\n",
      "[Epoch 276/2000] Train Loss: 1799.200256 | Val Loss: 1825.224731\n",
      "[Epoch 277/2000] Train Loss: 1797.496964 | Val Loss: 1821.819092\n",
      "[Epoch 278/2000] Train Loss: 1795.068771 | Val Loss: 1822.688965\n",
      "[Epoch 279/2000] Train Loss: 1796.594208 | Val Loss: 1824.105957\n",
      "[Epoch 280/2000] Train Loss: 1793.302063 | Val Loss: 1822.606079\n",
      "[Epoch 281/2000] Train Loss: 1792.478104 | Val Loss: 1821.956787\n",
      "[Epoch 282/2000] Train Loss: 1791.656525 | Val Loss: 1814.924438\n",
      "[Epoch 283/2000] Train Loss: 1789.615250 | Val Loss: 1812.928101\n",
      "[Epoch 284/2000] Train Loss: 1789.492798 | Val Loss: 1815.259277\n",
      "[Epoch 285/2000] Train Loss: 1788.183441 | Val Loss: 1815.204590\n",
      "[Epoch 286/2000] Train Loss: 1785.953827 | Val Loss: 1813.138916\n",
      "[Epoch 287/2000] Train Loss: 1784.137085 | Val Loss: 1807.153931\n",
      "[Epoch 288/2000] Train Loss: 1785.241898 | Val Loss: 1806.401001\n",
      "[Epoch 289/2000] Train Loss: 1783.187958 | Val Loss: 1808.348267\n",
      "[Epoch 290/2000] Train Loss: 1782.519882 | Val Loss: 1815.452148\n",
      "[Epoch 291/2000] Train Loss: 1781.123184 | Val Loss: 1811.755371\n",
      "[Epoch 292/2000] Train Loss: 1777.431351 | Val Loss: 1802.473267\n",
      "[Epoch 293/2000] Train Loss: 1777.748611 | Val Loss: 1799.775391\n",
      "[Epoch 294/2000] Train Loss: 1777.135941 | Val Loss: 1801.500366\n",
      "[Epoch 295/2000] Train Loss: 1774.978973 | Val Loss: 1802.531494\n",
      "[Epoch 296/2000] Train Loss: 1773.308182 | Val Loss: 1805.050049\n",
      "[Epoch 297/2000] Train Loss: 1773.161484 | Val Loss: 1799.162354\n",
      "[Epoch 298/2000] Train Loss: 1773.535202 | Val Loss: 1797.294678\n",
      "[Epoch 299/2000] Train Loss: 1771.948166 | Val Loss: 1793.735107\n",
      "[Epoch 300/2000] Train Loss: 1769.259842 | Val Loss: 1797.327515\n",
      "[Epoch 301/2000] Train Loss: 1768.114105 | Val Loss: 1799.900024\n",
      "[Epoch 302/2000] Train Loss: 1766.921722 | Val Loss: 1795.543213\n",
      "[Epoch 303/2000] Train Loss: 1766.735855 | Val Loss: 1787.008057\n",
      "[Epoch 304/2000] Train Loss: 1763.002548 | Val Loss: 1788.304810\n",
      "[Epoch 305/2000] Train Loss: 1765.273666 | Val Loss: 1790.307373\n",
      "[Epoch 306/2000] Train Loss: 1763.118759 | Val Loss: 1786.230103\n",
      "[Epoch 307/2000] Train Loss: 1760.859985 | Val Loss: 1787.911133\n",
      "[Epoch 308/2000] Train Loss: 1761.579773 | Val Loss: 1789.326660\n",
      "[Epoch 309/2000] Train Loss: 1756.724945 | Val Loss: 1783.391235\n",
      "[Epoch 310/2000] Train Loss: 1757.547455 | Val Loss: 1780.418457\n",
      "[Epoch 311/2000] Train Loss: 1757.823792 | Val Loss: 1781.524170\n",
      "[Epoch 312/2000] Train Loss: 1754.624878 | Val Loss: 1777.338501\n",
      "[Epoch 313/2000] Train Loss: 1752.701523 | Val Loss: 1778.608154\n",
      "[Epoch 314/2000] Train Loss: 1751.344925 | Val Loss: 1776.287231\n",
      "[Epoch 315/2000] Train Loss: 1751.761719 | Val Loss: 1776.833496\n",
      "[Epoch 316/2000] Train Loss: 1749.215912 | Val Loss: 1778.452148\n",
      "[Epoch 317/2000] Train Loss: 1749.021164 | Val Loss: 1780.327393\n",
      "[Epoch 318/2000] Train Loss: 1747.515076 | Val Loss: 1771.396729\n",
      "[Epoch 319/2000] Train Loss: 1745.929016 | Val Loss: 1767.764648\n",
      "[Epoch 320/2000] Train Loss: 1744.087906 | Val Loss: 1763.137329\n",
      "[Epoch 321/2000] Train Loss: 1743.421967 | Val Loss: 1766.049927\n",
      "[Epoch 322/2000] Train Loss: 1741.816422 | Val Loss: 1759.572632\n",
      "[Epoch 323/2000] Train Loss: 1740.126541 | Val Loss: 1765.943970\n",
      "[Epoch 324/2000] Train Loss: 1739.334045 | Val Loss: 1767.271240\n",
      "[Epoch 325/2000] Train Loss: 1740.981003 | Val Loss: 1772.299561\n",
      "[Epoch 326/2000] Train Loss: 1734.232285 | Val Loss: 1768.083130\n",
      "[Epoch 327/2000] Train Loss: 1735.313538 | Val Loss: 1766.185669\n",
      "[Epoch 328/2000] Train Loss: 1734.882721 | Val Loss: 1761.712036\n",
      "[Epoch 329/2000] Train Loss: 1732.759445 | Val Loss: 1755.047241\n",
      "[Epoch 330/2000] Train Loss: 1732.383545 | Val Loss: 1761.673340\n",
      "[Epoch 331/2000] Train Loss: 1730.683792 | Val Loss: 1760.852783\n",
      "[Epoch 332/2000] Train Loss: 1729.884552 | Val Loss: 1752.365845\n",
      "[Epoch 333/2000] Train Loss: 1728.686157 | Val Loss: 1748.459473\n",
      "[Epoch 334/2000] Train Loss: 1724.976532 | Val Loss: 1745.757568\n",
      "[Epoch 335/2000] Train Loss: 1724.613144 | Val Loss: 1742.757690\n",
      "[Epoch 336/2000] Train Loss: 1725.999756 | Val Loss: 1741.101318\n",
      "[Epoch 337/2000] Train Loss: 1722.684296 | Val Loss: 1744.406982\n",
      "[Epoch 338/2000] Train Loss: 1723.214081 | Val Loss: 1734.326538\n",
      "[Epoch 339/2000] Train Loss: 1719.943146 | Val Loss: 1733.213257\n",
      "[Epoch 340/2000] Train Loss: 1719.014557 | Val Loss: 1733.614746\n",
      "[Epoch 341/2000] Train Loss: 1718.490631 | Val Loss: 1738.677246\n",
      "[Epoch 342/2000] Train Loss: 1715.748642 | Val Loss: 1737.420532\n",
      "[Epoch 343/2000] Train Loss: 1715.745621 | Val Loss: 1738.075928\n",
      "[Epoch 344/2000] Train Loss: 1715.312500 | Val Loss: 1739.087402\n",
      "[Epoch 345/2000] Train Loss: 1712.813248 | Val Loss: 1736.948120\n",
      "[Epoch 346/2000] Train Loss: 1712.140091 | Val Loss: 1743.611084\n",
      "[Epoch 347/2000] Train Loss: 1707.940613 | Val Loss: 1734.252319\n",
      "[Epoch 348/2000] Train Loss: 1707.681488 | Val Loss: 1728.841309\n",
      "[Epoch 349/2000] Train Loss: 1706.376999 | Val Loss: 1729.543457\n",
      "[Epoch 350/2000] Train Loss: 1704.137177 | Val Loss: 1724.929199\n",
      "[Epoch 351/2000] Train Loss: 1703.094742 | Val Loss: 1724.764771\n",
      "[Epoch 352/2000] Train Loss: 1704.410568 | Val Loss: 1730.656494\n",
      "[Epoch 353/2000] Train Loss: 1702.713974 | Val Loss: 1726.796143\n",
      "[Epoch 354/2000] Train Loss: 1701.649551 | Val Loss: 1724.742554\n",
      "[Epoch 355/2000] Train Loss: 1698.959290 | Val Loss: 1723.519043\n",
      "[Epoch 356/2000] Train Loss: 1696.285522 | Val Loss: 1717.796265\n",
      "[Epoch 357/2000] Train Loss: 1696.558167 | Val Loss: 1718.265869\n",
      "[Epoch 358/2000] Train Loss: 1695.111038 | Val Loss: 1724.831543\n",
      "[Epoch 359/2000] Train Loss: 1695.680756 | Val Loss: 1720.297485\n",
      "[Epoch 360/2000] Train Loss: 1695.199844 | Val Loss: 1724.760986\n",
      "[Epoch 361/2000] Train Loss: 1688.935318 | Val Loss: 1718.235107\n",
      "[Epoch 362/2000] Train Loss: 1689.950165 | Val Loss: 1711.041138\n",
      "[Epoch 363/2000] Train Loss: 1687.616608 | Val Loss: 1711.940674\n",
      "[Epoch 364/2000] Train Loss: 1686.931778 | Val Loss: 1708.872925\n",
      "[Epoch 365/2000] Train Loss: 1685.826187 | Val Loss: 1702.764893\n",
      "[Epoch 366/2000] Train Loss: 1684.155945 | Val Loss: 1702.967651\n",
      "[Epoch 367/2000] Train Loss: 1682.734756 | Val Loss: 1705.269897\n",
      "[Epoch 368/2000] Train Loss: 1680.126694 | Val Loss: 1699.765747\n",
      "[Epoch 369/2000] Train Loss: 1679.145325 | Val Loss: 1704.262939\n",
      "[Epoch 370/2000] Train Loss: 1679.191238 | Val Loss: 1709.314941\n",
      "[Epoch 371/2000] Train Loss: 1677.784882 | Val Loss: 1703.656860\n",
      "[Epoch 372/2000] Train Loss: 1677.552490 | Val Loss: 1700.770386\n",
      "[Epoch 373/2000] Train Loss: 1675.052460 | Val Loss: 1699.390503\n",
      "[Epoch 374/2000] Train Loss: 1673.810791 | Val Loss: 1698.936768\n",
      "[Epoch 375/2000] Train Loss: 1673.151123 | Val Loss: 1696.988037\n",
      "[Epoch 376/2000] Train Loss: 1670.283600 | Val Loss: 1702.071167\n",
      "[Epoch 377/2000] Train Loss: 1668.677094 | Val Loss: 1695.054932\n",
      "[Epoch 378/2000] Train Loss: 1666.877945 | Val Loss: 1686.122803\n",
      "[Epoch 379/2000] Train Loss: 1666.454926 | Val Loss: 1689.676392\n",
      "[Epoch 380/2000] Train Loss: 1664.148178 | Val Loss: 1688.860229\n",
      "[Epoch 381/2000] Train Loss: 1663.911819 | Val Loss: 1689.451294\n",
      "[Epoch 382/2000] Train Loss: 1661.759399 | Val Loss: 1691.943481\n",
      "[Epoch 383/2000] Train Loss: 1659.579758 | Val Loss: 1686.447144\n",
      "[Epoch 384/2000] Train Loss: 1659.389877 | Val Loss: 1680.871460\n",
      "[Epoch 385/2000] Train Loss: 1656.887039 | Val Loss: 1682.320923\n",
      "[Epoch 386/2000] Train Loss: 1656.660263 | Val Loss: 1671.181152\n",
      "[Epoch 387/2000] Train Loss: 1653.992157 | Val Loss: 1668.169312\n",
      "[Epoch 388/2000] Train Loss: 1651.908829 | Val Loss: 1672.997192\n",
      "[Epoch 389/2000] Train Loss: 1656.019669 | Val Loss: 1672.705566\n",
      "[Epoch 390/2000] Train Loss: 1652.886841 | Val Loss: 1679.559326\n",
      "[Epoch 391/2000] Train Loss: 1648.874329 | Val Loss: 1672.180298\n",
      "[Epoch 392/2000] Train Loss: 1648.475891 | Val Loss: 1679.207153\n",
      "[Epoch 393/2000] Train Loss: 1647.265381 | Val Loss: 1674.929810\n",
      "[Epoch 394/2000] Train Loss: 1645.328415 | Val Loss: 1667.060791\n",
      "[Epoch 395/2000] Train Loss: 1643.308884 | Val Loss: 1669.149902\n",
      "[Epoch 396/2000] Train Loss: 1643.662735 | Val Loss: 1663.996460\n",
      "[Epoch 397/2000] Train Loss: 1639.287689 | Val Loss: 1659.119019\n",
      "[Epoch 398/2000] Train Loss: 1639.519073 | Val Loss: 1662.328003\n",
      "[Epoch 399/2000] Train Loss: 1637.558929 | Val Loss: 1664.523560\n",
      "[Epoch 400/2000] Train Loss: 1635.478912 | Val Loss: 1661.289795\n",
      "[Epoch 401/2000] Train Loss: 1634.986023 | Val Loss: 1653.833618\n",
      "[Epoch 402/2000] Train Loss: 1633.173157 | Val Loss: 1648.657471\n",
      "[Epoch 403/2000] Train Loss: 1631.378403 | Val Loss: 1649.184937\n",
      "[Epoch 404/2000] Train Loss: 1632.842804 | Val Loss: 1651.937622\n",
      "[Epoch 405/2000] Train Loss: 1628.125275 | Val Loss: 1650.842407\n",
      "[Epoch 406/2000] Train Loss: 1627.843277 | Val Loss: 1647.179443\n",
      "[Epoch 407/2000] Train Loss: 1625.784103 | Val Loss: 1643.514282\n",
      "[Epoch 408/2000] Train Loss: 1626.945007 | Val Loss: 1646.821533\n",
      "[Epoch 409/2000] Train Loss: 1624.359985 | Val Loss: 1643.016724\n",
      "[Epoch 410/2000] Train Loss: 1620.126465 | Val Loss: 1640.848145\n",
      "[Epoch 411/2000] Train Loss: 1619.875656 | Val Loss: 1641.007324\n",
      "[Epoch 412/2000] Train Loss: 1619.072205 | Val Loss: 1639.615967\n",
      "[Epoch 413/2000] Train Loss: 1618.051880 | Val Loss: 1634.377075\n",
      "[Epoch 414/2000] Train Loss: 1618.173004 | Val Loss: 1643.326782\n",
      "[Epoch 415/2000] Train Loss: 1615.910355 | Val Loss: 1640.185303\n",
      "[Epoch 416/2000] Train Loss: 1614.198944 | Val Loss: 1643.232544\n",
      "[Epoch 417/2000] Train Loss: 1610.803299 | Val Loss: 1639.457642\n",
      "[Epoch 418/2000] Train Loss: 1611.072983 | Val Loss: 1628.540161\n",
      "[Epoch 419/2000] Train Loss: 1608.430923 | Val Loss: 1637.514282\n",
      "[Epoch 420/2000] Train Loss: 1609.068680 | Val Loss: 1634.732056\n",
      "[Epoch 421/2000] Train Loss: 1606.587158 | Val Loss: 1632.155640\n",
      "[Epoch 422/2000] Train Loss: 1605.835144 | Val Loss: 1630.986816\n",
      "[Epoch 423/2000] Train Loss: 1602.714706 | Val Loss: 1623.376221\n",
      "[Epoch 424/2000] Train Loss: 1602.713165 | Val Loss: 1621.022339\n",
      "[Epoch 425/2000] Train Loss: 1599.978714 | Val Loss: 1617.282104\n",
      "[Epoch 426/2000] Train Loss: 1598.309479 | Val Loss: 1617.437012\n",
      "[Epoch 427/2000] Train Loss: 1597.835815 | Val Loss: 1615.553955\n",
      "[Epoch 428/2000] Train Loss: 1596.889053 | Val Loss: 1615.901855\n",
      "[Epoch 429/2000] Train Loss: 1594.628433 | Val Loss: 1614.988647\n",
      "[Epoch 430/2000] Train Loss: 1593.200394 | Val Loss: 1615.864990\n",
      "[Epoch 431/2000] Train Loss: 1591.071701 | Val Loss: 1609.828735\n",
      "[Epoch 432/2000] Train Loss: 1591.930649 | Val Loss: 1615.116821\n",
      "[Epoch 433/2000] Train Loss: 1588.638916 | Val Loss: 1609.624268\n",
      "[Epoch 434/2000] Train Loss: 1588.017471 | Val Loss: 1611.237671\n",
      "[Epoch 435/2000] Train Loss: 1586.657333 | Val Loss: 1610.128418\n",
      "[Epoch 436/2000] Train Loss: 1584.807373 | Val Loss: 1609.252075\n",
      "[Epoch 437/2000] Train Loss: 1582.134399 | Val Loss: 1611.468262\n",
      "[Epoch 438/2000] Train Loss: 1581.425674 | Val Loss: 1606.561279\n",
      "[Epoch 439/2000] Train Loss: 1578.481506 | Val Loss: 1603.431396\n",
      "[Epoch 440/2000] Train Loss: 1578.067215 | Val Loss: 1596.602783\n",
      "[Epoch 441/2000] Train Loss: 1578.031540 | Val Loss: 1607.857666\n",
      "[Epoch 442/2000] Train Loss: 1577.018417 | Val Loss: 1599.875854\n",
      "[Epoch 443/2000] Train Loss: 1573.890305 | Val Loss: 1595.738037\n",
      "[Epoch 444/2000] Train Loss: 1573.447235 | Val Loss: 1588.921143\n",
      "[Epoch 445/2000] Train Loss: 1572.397018 | Val Loss: 1592.309082\n",
      "[Epoch 446/2000] Train Loss: 1571.152847 | Val Loss: 1593.834229\n",
      "[Epoch 447/2000] Train Loss: 1568.713226 | Val Loss: 1592.565063\n",
      "[Epoch 448/2000] Train Loss: 1566.726074 | Val Loss: 1588.254150\n",
      "[Epoch 449/2000] Train Loss: 1564.475769 | Val Loss: 1581.472168\n",
      "[Epoch 450/2000] Train Loss: 1563.112518 | Val Loss: 1586.784424\n",
      "[Epoch 451/2000] Train Loss: 1562.095718 | Val Loss: 1580.215210\n",
      "[Epoch 452/2000] Train Loss: 1561.493576 | Val Loss: 1575.371948\n",
      "[Epoch 453/2000] Train Loss: 1558.449539 | Val Loss: 1574.088379\n",
      "[Epoch 454/2000] Train Loss: 1560.229813 | Val Loss: 1582.039429\n",
      "[Epoch 455/2000] Train Loss: 1556.626572 | Val Loss: 1575.875488\n",
      "[Epoch 456/2000] Train Loss: 1554.433044 | Val Loss: 1574.170044\n",
      "[Epoch 457/2000] Train Loss: 1553.945709 | Val Loss: 1575.491943\n",
      "[Epoch 458/2000] Train Loss: 1552.401382 | Val Loss: 1577.347900\n",
      "[Epoch 459/2000] Train Loss: 1552.124481 | Val Loss: 1573.640381\n",
      "[Epoch 460/2000] Train Loss: 1548.492523 | Val Loss: 1570.012451\n",
      "[Epoch 461/2000] Train Loss: 1548.466080 | Val Loss: 1576.279541\n",
      "[Epoch 462/2000] Train Loss: 1546.429749 | Val Loss: 1568.603271\n",
      "[Epoch 463/2000] Train Loss: 1543.880356 | Val Loss: 1562.060303\n",
      "[Epoch 464/2000] Train Loss: 1542.852539 | Val Loss: 1561.101685\n",
      "[Epoch 465/2000] Train Loss: 1541.623825 | Val Loss: 1561.286499\n",
      "[Epoch 466/2000] Train Loss: 1538.807327 | Val Loss: 1561.289429\n",
      "[Epoch 467/2000] Train Loss: 1539.000626 | Val Loss: 1564.333252\n",
      "[Epoch 468/2000] Train Loss: 1536.756470 | Val Loss: 1557.951538\n",
      "[Epoch 469/2000] Train Loss: 1535.460907 | Val Loss: 1552.676025\n",
      "[Epoch 470/2000] Train Loss: 1532.381256 | Val Loss: 1545.795044\n",
      "[Epoch 471/2000] Train Loss: 1530.334381 | Val Loss: 1544.445923\n",
      "[Epoch 472/2000] Train Loss: 1530.576523 | Val Loss: 1548.286377\n",
      "[Epoch 473/2000] Train Loss: 1530.369095 | Val Loss: 1554.120972\n",
      "[Epoch 474/2000] Train Loss: 1529.638321 | Val Loss: 1556.735107\n",
      "[Epoch 475/2000] Train Loss: 1525.037247 | Val Loss: 1545.653564\n",
      "[Epoch 476/2000] Train Loss: 1524.016388 | Val Loss: 1535.707153\n",
      "[Epoch 477/2000] Train Loss: 1522.774368 | Val Loss: 1538.822388\n",
      "[Epoch 478/2000] Train Loss: 1521.544159 | Val Loss: 1536.346313\n",
      "[Epoch 479/2000] Train Loss: 1518.239868 | Val Loss: 1532.848999\n",
      "[Epoch 480/2000] Train Loss: 1517.863998 | Val Loss: 1528.854370\n",
      "[Epoch 481/2000] Train Loss: 1517.774155 | Val Loss: 1534.286377\n",
      "[Epoch 482/2000] Train Loss: 1514.731140 | Val Loss: 1537.372803\n",
      "[Epoch 483/2000] Train Loss: 1514.678528 | Val Loss: 1541.648560\n",
      "[Epoch 484/2000] Train Loss: 1510.780670 | Val Loss: 1539.705933\n",
      "[Epoch 485/2000] Train Loss: 1510.453018 | Val Loss: 1545.659668\n",
      "[Epoch 486/2000] Train Loss: 1510.422379 | Val Loss: 1535.404663\n",
      "[Epoch 487/2000] Train Loss: 1507.308197 | Val Loss: 1527.203247\n",
      "[Epoch 488/2000] Train Loss: 1506.502197 | Val Loss: 1529.160522\n",
      "[Epoch 489/2000] Train Loss: 1504.212173 | Val Loss: 1520.044678\n",
      "[Epoch 490/2000] Train Loss: 1503.983078 | Val Loss: 1528.762817\n",
      "[Epoch 491/2000] Train Loss: 1501.678589 | Val Loss: 1524.769653\n",
      "[Epoch 492/2000] Train Loss: 1497.594757 | Val Loss: 1521.260254\n",
      "[Epoch 493/2000] Train Loss: 1497.874893 | Val Loss: 1518.418091\n",
      "[Epoch 494/2000] Train Loss: 1497.048218 | Val Loss: 1519.973755\n",
      "[Epoch 495/2000] Train Loss: 1496.241669 | Val Loss: 1515.537476\n",
      "[Epoch 496/2000] Train Loss: 1493.492752 | Val Loss: 1512.885132\n",
      "[Epoch 497/2000] Train Loss: 1493.067093 | Val Loss: 1512.092651\n",
      "[Epoch 498/2000] Train Loss: 1491.676773 | Val Loss: 1511.897827\n",
      "[Epoch 499/2000] Train Loss: 1489.903549 | Val Loss: 1514.903198\n",
      "[Epoch 500/2000] Train Loss: 1487.374023 | Val Loss: 1506.411987\n",
      "[Epoch 501/2000] Train Loss: 1486.611084 | Val Loss: 1508.999512\n",
      "[Epoch 502/2000] Train Loss: 1482.482834 | Val Loss: 1511.547974\n",
      "[Epoch 503/2000] Train Loss: 1481.420044 | Val Loss: 1505.422729\n",
      "[Epoch 504/2000] Train Loss: 1481.849503 | Val Loss: 1506.195801\n",
      "[Epoch 505/2000] Train Loss: 1480.855652 | Val Loss: 1507.823975\n",
      "[Epoch 506/2000] Train Loss: 1477.941925 | Val Loss: 1496.464966\n",
      "[Epoch 507/2000] Train Loss: 1476.777390 | Val Loss: 1500.429810\n",
      "[Epoch 508/2000] Train Loss: 1474.540405 | Val Loss: 1504.486816\n",
      "[Epoch 509/2000] Train Loss: 1474.311417 | Val Loss: 1501.375610\n",
      "[Epoch 510/2000] Train Loss: 1471.805664 | Val Loss: 1497.642456\n",
      "[Epoch 511/2000] Train Loss: 1468.423538 | Val Loss: 1491.447754\n",
      "[Epoch 512/2000] Train Loss: 1468.985016 | Val Loss: 1488.717163\n",
      "[Epoch 513/2000] Train Loss: 1467.117447 | Val Loss: 1479.701294\n",
      "[Epoch 514/2000] Train Loss: 1464.959656 | Val Loss: 1487.338257\n",
      "[Epoch 515/2000] Train Loss: 1462.886826 | Val Loss: 1482.531616\n",
      "[Epoch 516/2000] Train Loss: 1462.758423 | Val Loss: 1478.069580\n",
      "[Epoch 517/2000] Train Loss: 1460.594116 | Val Loss: 1476.788696\n",
      "[Epoch 518/2000] Train Loss: 1460.290329 | Val Loss: 1478.941162\n",
      "[Epoch 519/2000] Train Loss: 1455.815643 | Val Loss: 1477.967529\n",
      "[Epoch 520/2000] Train Loss: 1455.369873 | Val Loss: 1482.800659\n",
      "[Epoch 521/2000] Train Loss: 1454.893692 | Val Loss: 1479.119995\n",
      "[Epoch 522/2000] Train Loss: 1452.475235 | Val Loss: 1477.263916\n",
      "[Epoch 523/2000] Train Loss: 1451.196289 | Val Loss: 1472.813354\n",
      "[Epoch 524/2000] Train Loss: 1448.841507 | Val Loss: 1471.452881\n",
      "[Epoch 525/2000] Train Loss: 1447.736801 | Val Loss: 1465.337036\n",
      "[Epoch 526/2000] Train Loss: 1445.124863 | Val Loss: 1470.655762\n",
      "[Epoch 527/2000] Train Loss: 1443.435883 | Val Loss: 1466.046265\n",
      "[Epoch 528/2000] Train Loss: 1443.396942 | Val Loss: 1462.546387\n",
      "[Epoch 529/2000] Train Loss: 1441.404587 | Val Loss: 1466.808716\n",
      "[Epoch 530/2000] Train Loss: 1442.517502 | Val Loss: 1464.725098\n",
      "[Epoch 531/2000] Train Loss: 1437.173370 | Val Loss: 1458.657837\n",
      "[Epoch 532/2000] Train Loss: 1436.611267 | Val Loss: 1460.320923\n",
      "[Epoch 533/2000] Train Loss: 1436.653687 | Val Loss: 1462.312622\n",
      "[Epoch 534/2000] Train Loss: 1433.408096 | Val Loss: 1459.477783\n",
      "[Epoch 535/2000] Train Loss: 1431.474243 | Val Loss: 1454.596436\n",
      "[Epoch 536/2000] Train Loss: 1431.942047 | Val Loss: 1448.587769\n",
      "[Epoch 537/2000] Train Loss: 1430.870682 | Val Loss: 1447.389282\n",
      "[Epoch 538/2000] Train Loss: 1427.617966 | Val Loss: 1450.027466\n",
      "[Epoch 539/2000] Train Loss: 1426.027664 | Val Loss: 1452.221069\n",
      "[Epoch 540/2000] Train Loss: 1423.107849 | Val Loss: 1449.020020\n",
      "[Epoch 541/2000] Train Loss: 1423.423035 | Val Loss: 1447.620361\n",
      "[Epoch 542/2000] Train Loss: 1420.253723 | Val Loss: 1446.881714\n",
      "[Epoch 543/2000] Train Loss: 1418.979126 | Val Loss: 1447.103271\n",
      "[Epoch 544/2000] Train Loss: 1418.804520 | Val Loss: 1441.937622\n",
      "[Epoch 545/2000] Train Loss: 1415.168488 | Val Loss: 1439.295166\n",
      "[Epoch 546/2000] Train Loss: 1414.006989 | Val Loss: 1440.359497\n",
      "[Epoch 547/2000] Train Loss: 1412.653809 | Val Loss: 1440.925293\n",
      "[Epoch 548/2000] Train Loss: 1411.492126 | Val Loss: 1435.531494\n",
      "[Epoch 549/2000] Train Loss: 1409.048874 | Val Loss: 1429.376221\n",
      "[Epoch 550/2000] Train Loss: 1408.266052 | Val Loss: 1429.803955\n",
      "[Epoch 551/2000] Train Loss: 1406.157608 | Val Loss: 1428.848511\n",
      "[Epoch 552/2000] Train Loss: 1405.567108 | Val Loss: 1433.526489\n",
      "[Epoch 553/2000] Train Loss: 1403.297806 | Val Loss: 1429.794678\n",
      "[Epoch 554/2000] Train Loss: 1402.504898 | Val Loss: 1430.779663\n",
      "[Epoch 555/2000] Train Loss: 1399.710037 | Val Loss: 1423.566284\n",
      "[Epoch 556/2000] Train Loss: 1397.294312 | Val Loss: 1411.978760\n",
      "[Epoch 557/2000] Train Loss: 1395.968582 | Val Loss: 1419.780396\n",
      "[Epoch 558/2000] Train Loss: 1394.573380 | Val Loss: 1417.165894\n",
      "[Epoch 559/2000] Train Loss: 1393.104919 | Val Loss: 1417.351440\n",
      "[Epoch 560/2000] Train Loss: 1391.903046 | Val Loss: 1416.597168\n",
      "[Epoch 561/2000] Train Loss: 1390.603378 | Val Loss: 1413.296997\n",
      "[Epoch 562/2000] Train Loss: 1388.761429 | Val Loss: 1413.315308\n",
      "[Epoch 563/2000] Train Loss: 1389.848160 | Val Loss: 1411.665527\n",
      "[Epoch 564/2000] Train Loss: 1384.965164 | Val Loss: 1404.802368\n",
      "[Epoch 565/2000] Train Loss: 1383.996475 | Val Loss: 1402.024048\n",
      "[Epoch 566/2000] Train Loss: 1382.797623 | Val Loss: 1407.761963\n",
      "[Epoch 567/2000] Train Loss: 1380.234421 | Val Loss: 1404.363403\n",
      "[Epoch 568/2000] Train Loss: 1378.425720 | Val Loss: 1399.543579\n",
      "[Epoch 569/2000] Train Loss: 1377.708160 | Val Loss: 1394.671143\n",
      "[Epoch 570/2000] Train Loss: 1375.507690 | Val Loss: 1393.459839\n",
      "[Epoch 571/2000] Train Loss: 1374.600433 | Val Loss: 1394.331787\n",
      "[Epoch 572/2000] Train Loss: 1371.112198 | Val Loss: 1386.761475\n",
      "[Epoch 573/2000] Train Loss: 1371.456131 | Val Loss: 1386.271606\n",
      "[Epoch 574/2000] Train Loss: 1368.691147 | Val Loss: 1384.822876\n",
      "[Epoch 575/2000] Train Loss: 1368.334259 | Val Loss: 1388.613770\n",
      "[Epoch 576/2000] Train Loss: 1365.554092 | Val Loss: 1390.673340\n",
      "[Epoch 577/2000] Train Loss: 1363.279556 | Val Loss: 1389.192261\n",
      "[Epoch 578/2000] Train Loss: 1362.268173 | Val Loss: 1387.361816\n",
      "[Epoch 579/2000] Train Loss: 1361.176147 | Val Loss: 1383.454346\n",
      "[Epoch 580/2000] Train Loss: 1358.648300 | Val Loss: 1383.151855\n",
      "[Epoch 581/2000] Train Loss: 1358.675522 | Val Loss: 1376.443237\n",
      "[Epoch 582/2000] Train Loss: 1357.789551 | Val Loss: 1378.272827\n",
      "[Epoch 583/2000] Train Loss: 1353.596985 | Val Loss: 1379.137817\n",
      "[Epoch 584/2000] Train Loss: 1353.721024 | Val Loss: 1379.354370\n",
      "[Epoch 585/2000] Train Loss: 1351.514130 | Val Loss: 1378.150513\n",
      "[Epoch 586/2000] Train Loss: 1350.324036 | Val Loss: 1375.310303\n",
      "[Epoch 587/2000] Train Loss: 1346.322784 | Val Loss: 1366.724854\n",
      "[Epoch 588/2000] Train Loss: 1346.321625 | Val Loss: 1368.055786\n",
      "[Epoch 589/2000] Train Loss: 1343.539078 | Val Loss: 1368.557251\n",
      "[Epoch 590/2000] Train Loss: 1340.624146 | Val Loss: 1366.385376\n",
      "[Epoch 591/2000] Train Loss: 1341.994965 | Val Loss: 1366.370972\n",
      "[Epoch 592/2000] Train Loss: 1339.209320 | Val Loss: 1359.188110\n",
      "[Epoch 593/2000] Train Loss: 1338.019745 | Val Loss: 1358.679199\n",
      "[Epoch 594/2000] Train Loss: 1335.602890 | Val Loss: 1354.530151\n",
      "[Epoch 595/2000] Train Loss: 1336.417984 | Val Loss: 1359.523438\n",
      "[Epoch 596/2000] Train Loss: 1333.173477 | Val Loss: 1355.079834\n",
      "[Epoch 597/2000] Train Loss: 1330.764893 | Val Loss: 1348.086914\n",
      "[Epoch 598/2000] Train Loss: 1329.359558 | Val Loss: 1349.138672\n",
      "[Epoch 599/2000] Train Loss: 1328.004410 | Val Loss: 1349.350342\n",
      "[Epoch 600/2000] Train Loss: 1326.856262 | Val Loss: 1345.193726\n",
      "[Epoch 601/2000] Train Loss: 1325.473282 | Val Loss: 1342.820557\n",
      "[Epoch 602/2000] Train Loss: 1323.751053 | Val Loss: 1345.124878\n",
      "[Epoch 603/2000] Train Loss: 1322.313293 | Val Loss: 1351.562256\n",
      "[Epoch 604/2000] Train Loss: 1320.450607 | Val Loss: 1347.936890\n",
      "[Epoch 605/2000] Train Loss: 1319.819916 | Val Loss: 1337.659546\n",
      "[Epoch 606/2000] Train Loss: 1319.424133 | Val Loss: 1339.684326\n",
      "[Epoch 607/2000] Train Loss: 1315.526672 | Val Loss: 1338.117554\n",
      "[Epoch 608/2000] Train Loss: 1313.107346 | Val Loss: 1341.332031\n",
      "[Epoch 609/2000] Train Loss: 1312.158585 | Val Loss: 1342.149902\n",
      "[Epoch 610/2000] Train Loss: 1309.071091 | Val Loss: 1334.846191\n",
      "[Epoch 611/2000] Train Loss: 1307.059280 | Val Loss: 1334.140869\n",
      "[Epoch 612/2000] Train Loss: 1306.106110 | Val Loss: 1330.384766\n",
      "[Epoch 613/2000] Train Loss: 1305.850983 | Val Loss: 1323.008179\n",
      "[Epoch 614/2000] Train Loss: 1302.987381 | Val Loss: 1326.693970\n",
      "[Epoch 615/2000] Train Loss: 1300.348694 | Val Loss: 1327.194214\n",
      "[Epoch 616/2000] Train Loss: 1299.420883 | Val Loss: 1332.003906\n",
      "[Epoch 617/2000] Train Loss: 1297.156754 | Val Loss: 1321.906860\n",
      "[Epoch 618/2000] Train Loss: 1296.772430 | Val Loss: 1320.184448\n",
      "[Epoch 619/2000] Train Loss: 1293.103073 | Val Loss: 1317.674194\n",
      "[Epoch 620/2000] Train Loss: 1294.418304 | Val Loss: 1308.453003\n",
      "[Epoch 621/2000] Train Loss: 1290.785172 | Val Loss: 1311.933838\n",
      "[Epoch 622/2000] Train Loss: 1288.769196 | Val Loss: 1312.016357\n",
      "[Epoch 623/2000] Train Loss: 1288.667526 | Val Loss: 1310.786499\n",
      "[Epoch 624/2000] Train Loss: 1287.972427 | Val Loss: 1316.472046\n",
      "[Epoch 625/2000] Train Loss: 1284.805801 | Val Loss: 1308.998413\n",
      "[Epoch 626/2000] Train Loss: 1284.546677 | Val Loss: 1310.220459\n",
      "[Epoch 627/2000] Train Loss: 1281.083252 | Val Loss: 1307.389526\n",
      "[Epoch 628/2000] Train Loss: 1280.499222 | Val Loss: 1308.001587\n",
      "[Epoch 629/2000] Train Loss: 1278.458511 | Val Loss: 1302.836548\n",
      "[Epoch 630/2000] Train Loss: 1276.675461 | Val Loss: 1306.157471\n",
      "[Epoch 631/2000] Train Loss: 1275.867111 | Val Loss: 1301.952393\n",
      "[Epoch 632/2000] Train Loss: 1273.385590 | Val Loss: 1297.740967\n",
      "[Epoch 633/2000] Train Loss: 1270.741745 | Val Loss: 1296.561523\n",
      "[Epoch 634/2000] Train Loss: 1269.167419 | Val Loss: 1295.113647\n",
      "[Epoch 635/2000] Train Loss: 1267.411301 | Val Loss: 1294.208984\n",
      "[Epoch 636/2000] Train Loss: 1265.936554 | Val Loss: 1288.803101\n",
      "[Epoch 637/2000] Train Loss: 1266.967743 | Val Loss: 1288.972290\n",
      "[Epoch 638/2000] Train Loss: 1264.162262 | Val Loss: 1283.358032\n",
      "[Epoch 639/2000] Train Loss: 1262.671524 | Val Loss: 1285.148682\n",
      "[Epoch 640/2000] Train Loss: 1260.149017 | Val Loss: 1285.106689\n",
      "[Epoch 641/2000] Train Loss: 1257.435287 | Val Loss: 1282.235229\n",
      "[Epoch 642/2000] Train Loss: 1256.011765 | Val Loss: 1279.447876\n",
      "[Epoch 643/2000] Train Loss: 1255.544281 | Val Loss: 1275.325073\n",
      "[Epoch 644/2000] Train Loss: 1254.215652 | Val Loss: 1279.286865\n",
      "[Epoch 645/2000] Train Loss: 1252.163803 | Val Loss: 1278.769043\n",
      "[Epoch 646/2000] Train Loss: 1249.481934 | Val Loss: 1276.516357\n",
      "[Epoch 647/2000] Train Loss: 1247.804306 | Val Loss: 1272.541016\n",
      "[Epoch 648/2000] Train Loss: 1246.239548 | Val Loss: 1273.963745\n",
      "[Epoch 649/2000] Train Loss: 1243.278214 | Val Loss: 1267.364502\n",
      "[Epoch 650/2000] Train Loss: 1243.068039 | Val Loss: 1265.539673\n",
      "[Epoch 651/2000] Train Loss: 1241.519180 | Val Loss: 1270.210571\n",
      "[Epoch 652/2000] Train Loss: 1239.079224 | Val Loss: 1270.587158\n",
      "[Epoch 653/2000] Train Loss: 1237.997833 | Val Loss: 1269.108154\n",
      "[Epoch 654/2000] Train Loss: 1236.364243 | Val Loss: 1267.743408\n",
      "[Epoch 655/2000] Train Loss: 1235.951111 | Val Loss: 1258.815674\n",
      "[Epoch 656/2000] Train Loss: 1234.456696 | Val Loss: 1266.095459\n",
      "[Epoch 657/2000] Train Loss: 1231.660370 | Val Loss: 1256.687012\n",
      "[Epoch 658/2000] Train Loss: 1230.301346 | Val Loss: 1264.017090\n",
      "[Epoch 659/2000] Train Loss: 1227.091034 | Val Loss: 1259.192017\n",
      "[Epoch 660/2000] Train Loss: 1225.071426 | Val Loss: 1258.387329\n",
      "[Epoch 661/2000] Train Loss: 1224.220825 | Val Loss: 1258.248657\n",
      "[Epoch 662/2000] Train Loss: 1222.417084 | Val Loss: 1247.799438\n",
      "[Epoch 663/2000] Train Loss: 1221.141998 | Val Loss: 1242.031006\n",
      "[Epoch 664/2000] Train Loss: 1218.695435 | Val Loss: 1240.997070\n",
      "[Epoch 665/2000] Train Loss: 1215.906387 | Val Loss: 1237.546265\n",
      "[Epoch 666/2000] Train Loss: 1216.030151 | Val Loss: 1237.939941\n",
      "[Epoch 667/2000] Train Loss: 1213.654099 | Val Loss: 1235.336792\n",
      "[Epoch 668/2000] Train Loss: 1212.317490 | Val Loss: 1239.755615\n",
      "[Epoch 669/2000] Train Loss: 1210.623489 | Val Loss: 1239.922607\n",
      "[Epoch 670/2000] Train Loss: 1209.307343 | Val Loss: 1235.471558\n",
      "[Epoch 671/2000] Train Loss: 1207.586945 | Val Loss: 1232.510986\n",
      "[Epoch 672/2000] Train Loss: 1206.182205 | Val Loss: 1234.644531\n",
      "[Epoch 673/2000] Train Loss: 1204.764709 | Val Loss: 1227.652100\n",
      "[Epoch 674/2000] Train Loss: 1202.856628 | Val Loss: 1222.188965\n",
      "[Epoch 675/2000] Train Loss: 1202.036575 | Val Loss: 1226.518799\n",
      "[Epoch 676/2000] Train Loss: 1200.535568 | Val Loss: 1226.184326\n",
      "[Epoch 677/2000] Train Loss: 1198.742996 | Val Loss: 1226.438110\n",
      "[Epoch 678/2000] Train Loss: 1195.758438 | Val Loss: 1221.116455\n",
      "[Epoch 679/2000] Train Loss: 1194.452499 | Val Loss: 1222.831299\n",
      "[Epoch 680/2000] Train Loss: 1193.501770 | Val Loss: 1226.627319\n",
      "[Epoch 681/2000] Train Loss: 1190.339127 | Val Loss: 1225.561768\n",
      "[Epoch 682/2000] Train Loss: 1190.565781 | Val Loss: 1221.169189\n",
      "[Epoch 683/2000] Train Loss: 1187.385559 | Val Loss: 1215.234985\n",
      "[Epoch 684/2000] Train Loss: 1184.943451 | Val Loss: 1211.268799\n",
      "[Epoch 685/2000] Train Loss: 1184.633728 | Val Loss: 1213.598022\n",
      "[Epoch 686/2000] Train Loss: 1182.214676 | Val Loss: 1216.905151\n",
      "[Epoch 687/2000] Train Loss: 1180.515808 | Val Loss: 1211.945679\n",
      "[Epoch 688/2000] Train Loss: 1179.672440 | Val Loss: 1210.951538\n",
      "[Epoch 689/2000] Train Loss: 1174.408188 | Val Loss: 1208.372559\n",
      "[Epoch 690/2000] Train Loss: 1176.453201 | Val Loss: 1210.083252\n",
      "[Epoch 691/2000] Train Loss: 1175.149948 | Val Loss: 1209.043945\n",
      "[Epoch 692/2000] Train Loss: 1172.329834 | Val Loss: 1203.808472\n",
      "[Epoch 693/2000] Train Loss: 1172.604874 | Val Loss: 1209.944214\n",
      "[Epoch 694/2000] Train Loss: 1169.587357 | Val Loss: 1205.040649\n",
      "[Epoch 695/2000] Train Loss: 1167.947311 | Val Loss: 1204.694946\n",
      "[Epoch 696/2000] Train Loss: 1165.859146 | Val Loss: 1204.139038\n",
      "[Epoch 697/2000] Train Loss: 1163.564774 | Val Loss: 1205.261108\n",
      "[Epoch 698/2000] Train Loss: 1162.677917 | Val Loss: 1199.301758\n",
      "[Epoch 699/2000] Train Loss: 1160.954361 | Val Loss: 1198.857666\n",
      "[Epoch 700/2000] Train Loss: 1160.215195 | Val Loss: 1198.398804\n",
      "[Epoch 701/2000] Train Loss: 1157.624664 | Val Loss: 1199.685303\n",
      "[Epoch 702/2000] Train Loss: 1156.166901 | Val Loss: 1195.140381\n",
      "[Epoch 703/2000] Train Loss: 1154.431442 | Val Loss: 1196.744141\n",
      "[Epoch 704/2000] Train Loss: 1152.552170 | Val Loss: 1195.513916\n",
      "[Epoch 705/2000] Train Loss: 1151.541061 | Val Loss: 1191.274780\n",
      "[Epoch 706/2000] Train Loss: 1148.751572 | Val Loss: 1193.733643\n",
      "[Epoch 707/2000] Train Loss: 1146.160202 | Val Loss: 1188.564209\n",
      "[Epoch 708/2000] Train Loss: 1144.964691 | Val Loss: 1186.655762\n",
      "[Epoch 709/2000] Train Loss: 1142.433090 | Val Loss: 1184.807495\n",
      "[Epoch 710/2000] Train Loss: 1143.025742 | Val Loss: 1184.208496\n",
      "[Epoch 711/2000] Train Loss: 1138.849792 | Val Loss: 1191.854492\n",
      "[Epoch 712/2000] Train Loss: 1137.644196 | Val Loss: 1184.037476\n",
      "[Epoch 713/2000] Train Loss: 1136.655441 | Val Loss: 1185.937744\n",
      "[Epoch 714/2000] Train Loss: 1135.052719 | Val Loss: 1187.366089\n",
      "[Epoch 715/2000] Train Loss: 1132.429688 | Val Loss: 1185.509277\n",
      "[Epoch 716/2000] Train Loss: 1132.268661 | Val Loss: 1184.364136\n",
      "[Epoch 717/2000] Train Loss: 1130.043930 | Val Loss: 1186.915039\n",
      "[Epoch 718/2000] Train Loss: 1128.886810 | Val Loss: 1184.372559\n",
      "[Epoch 719/2000] Train Loss: 1128.156784 | Val Loss: 1192.391479\n",
      "[Epoch 720/2000] Train Loss: 1124.785797 | Val Loss: 1195.642578\n",
      "[Epoch 721/2000] Train Loss: 1123.577438 | Val Loss: 1200.796631\n",
      "[Epoch 722/2000] Train Loss: 1119.741928 | Val Loss: 1204.499756\n",
      "[Epoch 723/2000] Train Loss: 1119.860352 | Val Loss: 1206.164673\n",
      "[Epoch 724/2000] Train Loss: 1115.691360 | Val Loss: 1204.513672\n",
      "[Epoch 725/2000] Train Loss: 1115.983215 | Val Loss: 1221.485840\n",
      "[Epoch 726/2000] Train Loss: 1114.668701 | Val Loss: 1231.143799\n",
      "[Epoch 727/2000] Train Loss: 1113.270035 | Val Loss: 1237.062134\n",
      "[Epoch 728/2000] Train Loss: 1111.037994 | Val Loss: 1253.238770\n",
      "[Epoch 729/2000] Train Loss: 1108.194611 | Val Loss: 1293.080933\n",
      "[Epoch 730/2000] Train Loss: 1106.558044 | Val Loss: 1339.257568\n",
      "[Epoch 731/2000] Train Loss: 1104.064438 | Val Loss: 1411.426392\n",
      "[Epoch 732/2000] Train Loss: 1102.435898 | Val Loss: 1549.164673\n",
      "[Epoch 733/2000] Train Loss: 1098.120041 | Val Loss: 1780.413818\n",
      "[Epoch 734/2000] Train Loss: 1094.144745 | Val Loss: 1892.422485\n",
      "[Epoch 735/2000] Train Loss: 1092.560196 | Val Loss: 1588.238647\n",
      "[Epoch 736/2000] Train Loss: 1088.301163 | Val Loss: 1360.181396\n",
      "[Epoch 737/2000] Train Loss: 1088.882385 | Val Loss: 1374.818115\n",
      "[Epoch 738/2000] Train Loss: 1086.642921 | Val Loss: 1205.276611\n",
      "[Epoch 739/2000] Train Loss: 1085.699020 | Val Loss: 1094.528198\n",
      "[Epoch 740/2000] Train Loss: 1080.619232 | Val Loss: 1130.476318\n",
      "[Epoch 741/2000] Train Loss: 1080.425049 | Val Loss: 1025.728882\n",
      "[Epoch 742/2000] Train Loss: 1078.231903 | Val Loss: 1059.682007\n",
      "[Epoch 743/2000] Train Loss: 1074.931526 | Val Loss: 1102.438232\n",
      "[Epoch 744/2000] Train Loss: 1073.187149 | Val Loss: 1098.295654\n",
      "[Epoch 745/2000] Train Loss: 1074.244186 | Val Loss: 1018.983582\n",
      "[Epoch 746/2000] Train Loss: 1069.240906 | Val Loss: 1022.267334\n",
      "[Epoch 747/2000] Train Loss: 1069.566879 | Val Loss: 1111.022705\n",
      "[Epoch 748/2000] Train Loss: 1068.505180 | Val Loss: 1000.860107\n",
      "[Epoch 749/2000] Train Loss: 1067.946358 | Val Loss: 920.315308\n",
      "[Epoch 750/2000] Train Loss: 1064.009033 | Val Loss: 1136.964111\n",
      "[Epoch 751/2000] Train Loss: 1061.340897 | Val Loss: 1190.400269\n",
      "[Epoch 752/2000] Train Loss: 1061.099800 | Val Loss: 1007.568542\n",
      "[Epoch 753/2000] Train Loss: 1058.070587 | Val Loss: 996.930725\n",
      "[Epoch 754/2000] Train Loss: 1056.549225 | Val Loss: 1044.647949\n",
      "[Epoch 755/2000] Train Loss: 1055.511856 | Val Loss: 1114.143311\n",
      "[Epoch 756/2000] Train Loss: 1055.905975 | Val Loss: 1123.965210\n",
      "[Epoch 757/2000] Train Loss: 1050.227264 | Val Loss: 1008.341858\n",
      "[Epoch 758/2000] Train Loss: 1049.682053 | Val Loss: 1141.219971\n",
      "[Epoch 759/2000] Train Loss: 1048.232056 | Val Loss: 1016.508972\n",
      "[Epoch 760/2000] Train Loss: 1045.538246 | Val Loss: 1019.545288\n",
      "[Epoch 761/2000] Train Loss: 1046.077110 | Val Loss: 1011.212158\n",
      "[Epoch 762/2000] Train Loss: 1044.490761 | Val Loss: 1018.835693\n",
      "[Epoch 763/2000] Train Loss: 1040.788292 | Val Loss: 904.711243\n",
      "[Epoch 764/2000] Train Loss: 1040.421928 | Val Loss: 952.548035\n",
      "[Epoch 765/2000] Train Loss: 1038.415428 | Val Loss: 1060.097168\n",
      "[Epoch 766/2000] Train Loss: 1037.680855 | Val Loss: 1110.895508\n",
      "[Epoch 767/2000] Train Loss: 1033.722038 | Val Loss: 1023.516296\n",
      "[Epoch 768/2000] Train Loss: 1033.401260 | Val Loss: 970.737061\n",
      "[Epoch 769/2000] Train Loss: 1030.440445 | Val Loss: 983.231750\n",
      "[Epoch 770/2000] Train Loss: 1029.548401 | Val Loss: 1149.188599\n",
      "[Epoch 771/2000] Train Loss: 1029.706345 | Val Loss: 1036.653564\n",
      "[Epoch 772/2000] Train Loss: 1024.325096 | Val Loss: 961.618652\n",
      "[Epoch 773/2000] Train Loss: 1024.157539 | Val Loss: 1067.794922\n",
      "[Epoch 774/2000] Train Loss: 1023.174721 | Val Loss: 1005.172668\n",
      "[Epoch 775/2000] Train Loss: 1021.278152 | Val Loss: 996.665649\n",
      "[Epoch 776/2000] Train Loss: 1018.949654 | Val Loss: 1019.018555\n",
      "[Epoch 777/2000] Train Loss: 1018.620399 | Val Loss: 997.080872\n",
      "[Epoch 778/2000] Train Loss: 1017.133232 | Val Loss: 1023.789429\n",
      "[Epoch 779/2000] Train Loss: 1013.647690 | Val Loss: 987.774231\n",
      "[Epoch 780/2000] Train Loss: 1011.567001 | Val Loss: 1008.694885\n",
      "[Epoch 781/2000] Train Loss: 1009.018456 | Val Loss: 999.394653\n",
      "[Epoch 782/2000] Train Loss: 1009.305664 | Val Loss: 903.505859\n",
      "[Epoch 783/2000] Train Loss: 1006.522705 | Val Loss: 1011.249573\n",
      "[Epoch 784/2000] Train Loss: 1006.647469 | Val Loss: 987.915161\n",
      "[Epoch 785/2000] Train Loss: 1005.812965 | Val Loss: 1036.713379\n",
      "[Epoch 786/2000] Train Loss: 1001.479843 | Val Loss: 919.809875\n",
      "[Epoch 787/2000] Train Loss: 1001.058807 | Val Loss: 928.046509\n",
      "[Epoch 788/2000] Train Loss: 998.474167 | Val Loss: 993.926086\n",
      "[Epoch 789/2000] Train Loss: 997.269173 | Val Loss: 1048.002197\n",
      "[Epoch 790/2000] Train Loss: 993.400314 | Val Loss: 937.907898\n",
      "[Epoch 791/2000] Train Loss: 993.857262 | Val Loss: 966.004578\n",
      "[Epoch 792/2000] Train Loss: 993.095428 | Val Loss: 1145.935791\n",
      "[Epoch 793/2000] Train Loss: 989.410484 | Val Loss: 1059.605713\n",
      "[Epoch 794/2000] Train Loss: 989.559708 | Val Loss: 891.010071\n",
      "[Epoch 795/2000] Train Loss: 985.851067 | Val Loss: 912.691406\n",
      "[Epoch 796/2000] Train Loss: 985.492134 | Val Loss: 934.741394\n",
      "[Epoch 797/2000] Train Loss: 985.139488 | Val Loss: 1066.257812\n",
      "[Epoch 798/2000] Train Loss: 980.799881 | Val Loss: 1040.809570\n",
      "[Epoch 799/2000] Train Loss: 980.110413 | Val Loss: 916.156738\n",
      "[Epoch 800/2000] Train Loss: 979.643639 | Val Loss: 988.677368\n",
      "[Epoch 801/2000] Train Loss: 977.755623 | Val Loss: 853.612610\n",
      "[Epoch 802/2000] Train Loss: 974.856903 | Val Loss: 910.539551\n",
      "[Epoch 803/2000] Train Loss: 973.211784 | Val Loss: 1009.970581\n",
      "[Epoch 804/2000] Train Loss: 971.191437 | Val Loss: 1039.376831\n",
      "[Epoch 805/2000] Train Loss: 969.912598 | Val Loss: 925.280884\n",
      "[Epoch 806/2000] Train Loss: 969.428505 | Val Loss: 942.422180\n",
      "[Epoch 807/2000] Train Loss: 966.636208 | Val Loss: 1018.648804\n",
      "[Epoch 808/2000] Train Loss: 965.735847 | Val Loss: 934.809082\n",
      "[Epoch 809/2000] Train Loss: 964.119987 | Val Loss: 983.772461\n",
      "[Epoch 810/2000] Train Loss: 963.381660 | Val Loss: 1019.595215\n",
      "[Epoch 811/2000] Train Loss: 960.817261 | Val Loss: 962.776123\n",
      "[Epoch 812/2000] Train Loss: 959.254898 | Val Loss: 766.988892\n",
      "[Epoch 813/2000] Train Loss: 955.736290 | Val Loss: 814.350281\n",
      "[Epoch 814/2000] Train Loss: 953.772568 | Val Loss: 1005.748169\n",
      "[Epoch 815/2000] Train Loss: 952.759048 | Val Loss: 1037.838623\n",
      "[Epoch 816/2000] Train Loss: 950.737076 | Val Loss: 921.993652\n",
      "[Epoch 817/2000] Train Loss: 952.855431 | Val Loss: 926.540100\n",
      "[Epoch 818/2000] Train Loss: 949.746925 | Val Loss: 952.462769\n",
      "[Epoch 819/2000] Train Loss: 947.396255 | Val Loss: 999.535889\n",
      "[Epoch 820/2000] Train Loss: 944.391136 | Val Loss: 995.638000\n",
      "[Epoch 821/2000] Train Loss: 943.462212 | Val Loss: 1016.427246\n",
      "[Epoch 822/2000] Train Loss: 940.436073 | Val Loss: 816.992371\n",
      "[Epoch 823/2000] Train Loss: 939.275986 | Val Loss: 815.215759\n",
      "[Epoch 824/2000] Train Loss: 939.473175 | Val Loss: 977.364868\n",
      "[Epoch 825/2000] Train Loss: 939.059151 | Val Loss: 998.178101\n",
      "[Epoch 826/2000] Train Loss: 935.005348 | Val Loss: 955.073425\n",
      "[Epoch 827/2000] Train Loss: 932.751511 | Val Loss: 933.768066\n",
      "[Epoch 828/2000] Train Loss: 933.844658 | Val Loss: 973.405762\n",
      "[Epoch 829/2000] Train Loss: 929.932526 | Val Loss: 864.313354\n",
      "[Epoch 830/2000] Train Loss: 929.664948 | Val Loss: 801.204285\n",
      "[Epoch 831/2000] Train Loss: 926.285126 | Val Loss: 1015.022644\n",
      "[Epoch 832/2000] Train Loss: 925.413513 | Val Loss: 994.091064\n",
      "[Epoch 833/2000] Train Loss: 925.514679 | Val Loss: 859.825562\n",
      "[Epoch 834/2000] Train Loss: 922.513588 | Val Loss: 799.560852\n",
      "[Epoch 835/2000] Train Loss: 920.383125 | Val Loss: 944.994446\n",
      "[Epoch 836/2000] Train Loss: 918.662544 | Val Loss: 953.113525\n",
      "[Epoch 837/2000] Train Loss: 916.728287 | Val Loss: 987.937805\n",
      "[Epoch 838/2000] Train Loss: 916.041931 | Val Loss: 875.866028\n",
      "[Epoch 839/2000] Train Loss: 913.268250 | Val Loss: 855.789795\n",
      "[Epoch 840/2000] Train Loss: 910.024315 | Val Loss: 933.730286\n",
      "[Epoch 841/2000] Train Loss: 909.084595 | Val Loss: 926.943665\n",
      "[Epoch 842/2000] Train Loss: 909.583458 | Val Loss: 827.530884\n",
      "[Epoch 843/2000] Train Loss: 907.677788 | Val Loss: 795.802490\n",
      "[Epoch 844/2000] Train Loss: 904.685997 | Val Loss: 892.845764\n",
      "[Epoch 845/2000] Train Loss: 904.880127 | Val Loss: 934.199768\n",
      "[Epoch 846/2000] Train Loss: 903.400986 | Val Loss: 887.678345\n",
      "[Epoch 847/2000] Train Loss: 900.834488 | Val Loss: 931.242432\n",
      "[Epoch 848/2000] Train Loss: 898.609695 | Val Loss: 857.035583\n",
      "[Epoch 849/2000] Train Loss: 896.607628 | Val Loss: 938.977417\n",
      "[Epoch 850/2000] Train Loss: 897.298592 | Val Loss: 965.000854\n",
      "[Epoch 851/2000] Train Loss: 894.055763 | Val Loss: 888.276917\n",
      "[Epoch 852/2000] Train Loss: 893.097084 | Val Loss: 828.262573\n",
      "[Epoch 853/2000] Train Loss: 890.355164 | Val Loss: 858.625854\n",
      "[Epoch 854/2000] Train Loss: 890.579262 | Val Loss: 888.896606\n",
      "[Epoch 855/2000] Train Loss: 887.554779 | Val Loss: 930.672729\n",
      "[Epoch 856/2000] Train Loss: 885.629425 | Val Loss: 827.311279\n",
      "[Epoch 857/2000] Train Loss: 885.466270 | Val Loss: 832.752686\n",
      "[Epoch 858/2000] Train Loss: 881.346306 | Val Loss: 948.900757\n",
      "[Epoch 859/2000] Train Loss: 879.932968 | Val Loss: 876.248230\n",
      "[Epoch 860/2000] Train Loss: 875.727074 | Val Loss: 798.148865\n",
      "[Epoch 861/2000] Train Loss: 876.881554 | Val Loss: 844.517883\n",
      "[Epoch 862/2000] Train Loss: 874.131386 | Val Loss: 932.633545\n",
      "[Epoch 863/2000] Train Loss: 874.825272 | Val Loss: 952.038940\n",
      "[Epoch 864/2000] Train Loss: 871.221748 | Val Loss: 820.853210\n",
      "[Epoch 865/2000] Train Loss: 872.818138 | Val Loss: 760.746155\n",
      "[Epoch 866/2000] Train Loss: 868.558258 | Val Loss: 808.461121\n",
      "[Epoch 867/2000] Train Loss: 868.404175 | Val Loss: 873.009521\n",
      "[Epoch 868/2000] Train Loss: 866.862556 | Val Loss: 976.739441\n",
      "[Epoch 869/2000] Train Loss: 864.440147 | Val Loss: 936.350098\n",
      "[Epoch 870/2000] Train Loss: 862.347832 | Val Loss: 823.332703\n",
      "[Epoch 871/2000] Train Loss: 862.332558 | Val Loss: 738.769958\n",
      "[Epoch 872/2000] Train Loss: 859.793526 | Val Loss: 753.769592\n",
      "[Epoch 873/2000] Train Loss: 855.754280 | Val Loss: 901.549561\n",
      "[Epoch 874/2000] Train Loss: 856.504189 | Val Loss: 981.066162\n",
      "[Epoch 875/2000] Train Loss: 854.695496 | Val Loss: 845.671021\n",
      "[Epoch 876/2000] Train Loss: 851.775940 | Val Loss: 783.975220\n",
      "[Epoch 877/2000] Train Loss: 851.653786 | Val Loss: 827.837708\n",
      "[Epoch 878/2000] Train Loss: 848.815010 | Val Loss: 853.223694\n",
      "[Epoch 879/2000] Train Loss: 847.635872 | Val Loss: 821.299500\n",
      "[Epoch 880/2000] Train Loss: 846.774567 | Val Loss: 821.653503\n",
      "[Epoch 881/2000] Train Loss: 844.750221 | Val Loss: 818.860779\n",
      "[Epoch 882/2000] Train Loss: 843.190491 | Val Loss: 849.792419\n",
      "[Epoch 883/2000] Train Loss: 841.852409 | Val Loss: 823.492798\n",
      "[Epoch 884/2000] Train Loss: 838.937553 | Val Loss: 832.099243\n",
      "[Epoch 885/2000] Train Loss: 836.933754 | Val Loss: 821.578979\n",
      "[Epoch 886/2000] Train Loss: 838.079109 | Val Loss: 802.768860\n",
      "[Epoch 887/2000] Train Loss: 834.986839 | Val Loss: 799.583435\n",
      "[Epoch 888/2000] Train Loss: 833.864052 | Val Loss: 782.916199\n",
      "[Epoch 889/2000] Train Loss: 831.744431 | Val Loss: 934.457886\n",
      "[Epoch 890/2000] Train Loss: 831.729668 | Val Loss: 835.291443\n",
      "[Epoch 891/2000] Train Loss: 828.423317 | Val Loss: 797.223328\n",
      "[Epoch 892/2000] Train Loss: 826.837959 | Val Loss: 812.922485\n",
      "[Epoch 893/2000] Train Loss: 825.379707 | Val Loss: 826.248352\n",
      "[Epoch 894/2000] Train Loss: 824.324150 | Val Loss: 819.002258\n",
      "[Epoch 895/2000] Train Loss: 821.336685 | Val Loss: 903.715149\n",
      "[Epoch 896/2000] Train Loss: 820.105125 | Val Loss: 829.982544\n",
      "[Epoch 897/2000] Train Loss: 818.434769 | Val Loss: 827.715576\n",
      "[Epoch 898/2000] Train Loss: 817.624313 | Val Loss: 768.890686\n",
      "[Epoch 899/2000] Train Loss: 815.712494 | Val Loss: 753.098267\n",
      "[Epoch 900/2000] Train Loss: 814.031212 | Val Loss: 835.142029\n",
      "[Epoch 901/2000] Train Loss: 811.443184 | Val Loss: 841.592102\n",
      "[Epoch 902/2000] Train Loss: 812.409332 | Val Loss: 753.921204\n",
      "[Epoch 903/2000] Train Loss: 809.124382 | Val Loss: 681.922852\n",
      "[Epoch 904/2000] Train Loss: 807.588570 | Val Loss: 756.902405\n",
      "[Epoch 905/2000] Train Loss: 806.140984 | Val Loss: 885.482300\n",
      "[Epoch 906/2000] Train Loss: 803.010681 | Val Loss: 927.041748\n",
      "[Epoch 907/2000] Train Loss: 801.876556 | Val Loss: 876.254761\n",
      "[Epoch 908/2000] Train Loss: 799.970573 | Val Loss: 714.558044\n",
      "[Epoch 909/2000] Train Loss: 800.896240 | Val Loss: 693.277405\n",
      "[Epoch 910/2000] Train Loss: 797.422897 | Val Loss: 798.100464\n",
      "[Epoch 911/2000] Train Loss: 798.419632 | Val Loss: 839.294495\n",
      "[Epoch 912/2000] Train Loss: 795.675179 | Val Loss: 899.285767\n",
      "[Epoch 913/2000] Train Loss: 791.896416 | Val Loss: 815.949463\n",
      "[Epoch 914/2000] Train Loss: 791.475891 | Val Loss: 729.401550\n",
      "[Epoch 915/2000] Train Loss: 791.157532 | Val Loss: 784.956665\n",
      "[Epoch 916/2000] Train Loss: 787.970825 | Val Loss: 735.513000\n",
      "[Epoch 917/2000] Train Loss: 786.995903 | Val Loss: 762.579163\n",
      "[Epoch 918/2000] Train Loss: 786.172684 | Val Loss: 778.528564\n",
      "[Epoch 919/2000] Train Loss: 782.751526 | Val Loss: 831.254700\n",
      "[Epoch 920/2000] Train Loss: 783.207977 | Val Loss: 713.185852\n",
      "[Epoch 921/2000] Train Loss: 779.455544 | Val Loss: 776.330750\n",
      "[Epoch 922/2000] Train Loss: 778.235878 | Val Loss: 735.678772\n",
      "[Epoch 923/2000] Train Loss: 777.196205 | Val Loss: 795.807678\n",
      "[Epoch 924/2000] Train Loss: 774.203003 | Val Loss: 828.773621\n",
      "[Epoch 925/2000] Train Loss: 773.116295 | Val Loss: 744.825073\n",
      "[Epoch 926/2000] Train Loss: 772.102074 | Val Loss: 705.472900\n",
      "[Epoch 927/2000] Train Loss: 770.814026 | Val Loss: 742.835083\n",
      "[Epoch 928/2000] Train Loss: 769.572861 | Val Loss: 787.905884\n",
      "[Epoch 929/2000] Train Loss: 767.919952 | Val Loss: 736.550659\n",
      "[Epoch 930/2000] Train Loss: 765.354248 | Val Loss: 755.473083\n",
      "[Epoch 931/2000] Train Loss: 764.486450 | Val Loss: 715.079285\n",
      "[Epoch 932/2000] Train Loss: 762.413399 | Val Loss: 721.803772\n",
      "[Epoch 933/2000] Train Loss: 761.651688 | Val Loss: 861.467590\n",
      "[Epoch 934/2000] Train Loss: 759.915070 | Val Loss: 855.740051\n",
      "[Epoch 935/2000] Train Loss: 756.738403 | Val Loss: 710.854919\n",
      "[Epoch 936/2000] Train Loss: 755.664642 | Val Loss: 700.210022\n",
      "[Epoch 937/2000] Train Loss: 752.820908 | Val Loss: 733.694641\n",
      "[Epoch 938/2000] Train Loss: 753.850433 | Val Loss: 788.512146\n",
      "[Epoch 939/2000] Train Loss: 752.536781 | Val Loss: 799.918030\n",
      "[Epoch 940/2000] Train Loss: 748.780403 | Val Loss: 756.956360\n",
      "[Epoch 941/2000] Train Loss: 748.222656 | Val Loss: 717.848267\n",
      "[Epoch 942/2000] Train Loss: 746.321007 | Val Loss: 755.582642\n",
      "[Epoch 943/2000] Train Loss: 745.451065 | Val Loss: 753.910767\n",
      "[Epoch 944/2000] Train Loss: 744.565559 | Val Loss: 734.983704\n",
      "[Epoch 945/2000] Train Loss: 741.732262 | Val Loss: 747.028870\n",
      "[Epoch 946/2000] Train Loss: 741.253983 | Val Loss: 800.588684\n",
      "[Epoch 947/2000] Train Loss: 739.005196 | Val Loss: 720.121521\n",
      "[Epoch 948/2000] Train Loss: 735.795074 | Val Loss: 638.465210\n",
      "[Epoch 949/2000] Train Loss: 735.980690 | Val Loss: 713.380493\n",
      "[Epoch 950/2000] Train Loss: 736.304520 | Val Loss: 794.821350\n",
      "[Epoch 951/2000] Train Loss: 734.573425 | Val Loss: 752.471375\n",
      "[Epoch 952/2000] Train Loss: 730.709946 | Val Loss: 687.268738\n",
      "[Epoch 953/2000] Train Loss: 731.355431 | Val Loss: 648.785156\n",
      "[Epoch 954/2000] Train Loss: 727.703438 | Val Loss: 729.046570\n",
      "[Epoch 955/2000] Train Loss: 726.540337 | Val Loss: 768.583191\n",
      "[Epoch 956/2000] Train Loss: 725.543617 | Val Loss: 697.452332\n",
      "[Epoch 957/2000] Train Loss: 722.088921 | Val Loss: 735.642029\n",
      "[Epoch 958/2000] Train Loss: 721.766052 | Val Loss: 811.549561\n",
      "[Epoch 959/2000] Train Loss: 717.733131 | Val Loss: 795.811890\n",
      "[Epoch 960/2000] Train Loss: 715.311951 | Val Loss: 682.055847\n",
      "[Epoch 961/2000] Train Loss: 715.068001 | Val Loss: 663.597839\n",
      "[Epoch 962/2000] Train Loss: 711.846710 | Val Loss: 690.432678\n",
      "[Epoch 963/2000] Train Loss: 710.842476 | Val Loss: 714.815857\n",
      "[Epoch 964/2000] Train Loss: 709.096115 | Val Loss: 749.908936\n",
      "[Epoch 965/2000] Train Loss: 706.306839 | Val Loss: 762.580933\n",
      "[Epoch 966/2000] Train Loss: 707.079605 | Val Loss: 697.414978\n",
      "[Epoch 967/2000] Train Loss: 705.608864 | Val Loss: 668.072510\n",
      "[Epoch 968/2000] Train Loss: 702.141525 | Val Loss: 651.092102\n",
      "[Epoch 969/2000] Train Loss: 701.116554 | Val Loss: 683.874207\n",
      "[Epoch 970/2000] Train Loss: 698.656181 | Val Loss: 682.900024\n",
      "[Epoch 971/2000] Train Loss: 699.609581 | Val Loss: 621.501404\n",
      "[Epoch 972/2000] Train Loss: 695.256973 | Val Loss: 670.458496\n",
      "[Epoch 973/2000] Train Loss: 695.479866 | Val Loss: 674.768738\n",
      "[Epoch 974/2000] Train Loss: 694.436501 | Val Loss: 720.608337\n",
      "[Epoch 975/2000] Train Loss: 691.980881 | Val Loss: 693.607727\n",
      "[Epoch 976/2000] Train Loss: 689.534088 | Val Loss: 626.221436\n",
      "[Epoch 977/2000] Train Loss: 687.209076 | Val Loss: 624.619324\n",
      "[Epoch 978/2000] Train Loss: 686.602417 | Val Loss: 722.648193\n",
      "[Epoch 979/2000] Train Loss: 686.026985 | Val Loss: 759.398865\n",
      "[Epoch 980/2000] Train Loss: 684.807846 | Val Loss: 685.307251\n",
      "[Epoch 981/2000] Train Loss: 682.600082 | Val Loss: 657.458313\n",
      "[Epoch 982/2000] Train Loss: 682.180550 | Val Loss: 609.240967\n",
      "[Epoch 983/2000] Train Loss: 679.469063 | Val Loss: 652.817688\n",
      "[Epoch 984/2000] Train Loss: 678.263512 | Val Loss: 724.500916\n",
      "[Epoch 985/2000] Train Loss: 675.676857 | Val Loss: 706.574829\n",
      "[Epoch 986/2000] Train Loss: 674.912712 | Val Loss: 609.371460\n",
      "[Epoch 987/2000] Train Loss: 672.245621 | Val Loss: 641.223267\n",
      "[Epoch 988/2000] Train Loss: 671.289299 | Val Loss: 703.173889\n",
      "[Epoch 989/2000] Train Loss: 669.206306 | Val Loss: 673.985657\n",
      "[Epoch 990/2000] Train Loss: 669.242043 | Val Loss: 623.717102\n",
      "[Epoch 991/2000] Train Loss: 668.271591 | Val Loss: 651.985962\n",
      "[Epoch 992/2000] Train Loss: 666.689163 | Val Loss: 684.643982\n",
      "[Epoch 993/2000] Train Loss: 662.561409 | Val Loss: 667.628784\n",
      "[Epoch 994/2000] Train Loss: 661.300186 | Val Loss: 663.817200\n",
      "[Epoch 995/2000] Train Loss: 661.434364 | Val Loss: 674.961426\n",
      "[Epoch 996/2000] Train Loss: 659.575180 | Val Loss: 659.099548\n",
      "[Epoch 997/2000] Train Loss: 658.642563 | Val Loss: 603.338379\n",
      "[Epoch 998/2000] Train Loss: 658.458893 | Val Loss: 628.213196\n",
      "[Epoch 999/2000] Train Loss: 655.543777 | Val Loss: 664.119385\n",
      "[Epoch 1000/2000] Train Loss: 653.248291 | Val Loss: 625.508667\n",
      "[Epoch 1001/2000] Train Loss: 652.536957 | Val Loss: 613.900269\n",
      "[Epoch 1002/2000] Train Loss: 651.099358 | Val Loss: 646.802734\n",
      "[Epoch 1003/2000] Train Loss: 649.623009 | Val Loss: 636.264893\n",
      "[Epoch 1004/2000] Train Loss: 646.026909 | Val Loss: 650.889526\n",
      "[Epoch 1005/2000] Train Loss: 645.574348 | Val Loss: 646.254028\n",
      "[Epoch 1006/2000] Train Loss: 643.509705 | Val Loss: 694.629639\n",
      "[Epoch 1007/2000] Train Loss: 643.228592 | Val Loss: 697.622925\n",
      "[Epoch 1008/2000] Train Loss: 640.411926 | Val Loss: 577.837402\n",
      "[Epoch 1009/2000] Train Loss: 639.563614 | Val Loss: 549.317627\n",
      "[Epoch 1010/2000] Train Loss: 637.427528 | Val Loss: 626.303894\n",
      "[Epoch 1011/2000] Train Loss: 635.961105 | Val Loss: 658.392273\n",
      "[Epoch 1012/2000] Train Loss: 635.454567 | Val Loss: 664.818542\n",
      "[Epoch 1013/2000] Train Loss: 632.902191 | Val Loss: 590.539856\n",
      "[Epoch 1014/2000] Train Loss: 632.100487 | Val Loss: 655.107727\n",
      "[Epoch 1015/2000] Train Loss: 630.675026 | Val Loss: 601.888550\n",
      "[Epoch 1016/2000] Train Loss: 630.210121 | Val Loss: 592.552246\n",
      "[Epoch 1017/2000] Train Loss: 626.415886 | Val Loss: 624.495117\n",
      "[Epoch 1018/2000] Train Loss: 626.516258 | Val Loss: 638.650391\n",
      "[Epoch 1019/2000] Train Loss: 623.008705 | Val Loss: 607.344910\n",
      "[Epoch 1020/2000] Train Loss: 621.168655 | Val Loss: 556.053894\n",
      "[Epoch 1021/2000] Train Loss: 620.242203 | Val Loss: 606.173279\n",
      "[Epoch 1022/2000] Train Loss: 620.298203 | Val Loss: 644.947266\n",
      "[Epoch 1023/2000] Train Loss: 619.839661 | Val Loss: 648.403625\n",
      "[Epoch 1024/2000] Train Loss: 616.500206 | Val Loss: 612.315430\n",
      "[Epoch 1025/2000] Train Loss: 614.892761 | Val Loss: 623.752808\n",
      "[Epoch 1026/2000] Train Loss: 615.168892 | Val Loss: 616.887451\n",
      "[Epoch 1027/2000] Train Loss: 612.761192 | Val Loss: 608.799133\n",
      "[Epoch 1028/2000] Train Loss: 611.718071 | Val Loss: 556.421448\n",
      "[Epoch 1029/2000] Train Loss: 610.100433 | Val Loss: 607.966431\n",
      "[Epoch 1030/2000] Train Loss: 606.593979 | Val Loss: 687.223511\n",
      "[Epoch 1031/2000] Train Loss: 607.019516 | Val Loss: 686.604248\n",
      "[Epoch 1032/2000] Train Loss: 606.728706 | Val Loss: 593.215332\n",
      "[Epoch 1033/2000] Train Loss: 604.699303 | Val Loss: 559.335938\n",
      "[Epoch 1034/2000] Train Loss: 602.314507 | Val Loss: 562.752747\n",
      "[Epoch 1035/2000] Train Loss: 601.365593 | Val Loss: 664.025635\n",
      "[Epoch 1036/2000] Train Loss: 598.003899 | Val Loss: 692.581909\n",
      "[Epoch 1037/2000] Train Loss: 597.450760 | Val Loss: 648.705750\n",
      "[Epoch 1038/2000] Train Loss: 595.481148 | Val Loss: 549.707092\n",
      "[Epoch 1039/2000] Train Loss: 594.373383 | Val Loss: 587.994507\n",
      "[Epoch 1040/2000] Train Loss: 591.817787 | Val Loss: 646.484680\n",
      "[Epoch 1041/2000] Train Loss: 591.949760 | Val Loss: 716.058228\n",
      "[Epoch 1042/2000] Train Loss: 591.499428 | Val Loss: 682.553650\n",
      "[Epoch 1043/2000] Train Loss: 588.102020 | Val Loss: 617.884033\n",
      "[Epoch 1044/2000] Train Loss: 586.660843 | Val Loss: 619.947449\n",
      "[Epoch 1045/2000] Train Loss: 584.740524 | Val Loss: 648.307251\n",
      "[Epoch 1046/2000] Train Loss: 584.708755 | Val Loss: 721.746887\n",
      "[Epoch 1047/2000] Train Loss: 582.089500 | Val Loss: 734.175842\n",
      "[Epoch 1048/2000] Train Loss: 581.878357 | Val Loss: 697.928528\n",
      "[Epoch 1049/2000] Train Loss: 578.895805 | Val Loss: 788.993103\n",
      "[Epoch 1050/2000] Train Loss: 578.733177 | Val Loss: 727.296509\n",
      "[Epoch 1051/2000] Train Loss: 576.482880 | Val Loss: 647.691101\n",
      "[Epoch 1052/2000] Train Loss: 572.881302 | Val Loss: 568.040466\n",
      "[Epoch 1053/2000] Train Loss: 573.426285 | Val Loss: 526.635620\n",
      "[Epoch 1054/2000] Train Loss: 570.245392 | Val Loss: 531.686707\n",
      "[Epoch 1055/2000] Train Loss: 569.393879 | Val Loss: 562.925720\n",
      "[Epoch 1056/2000] Train Loss: 568.802170 | Val Loss: 559.425049\n",
      "[Epoch 1057/2000] Train Loss: 565.327797 | Val Loss: 488.072418\n",
      "[Epoch 1058/2000] Train Loss: 565.336891 | Val Loss: 535.190552\n",
      "[Epoch 1059/2000] Train Loss: 563.362743 | Val Loss: 627.139099\n",
      "[Epoch 1060/2000] Train Loss: 562.416607 | Val Loss: 614.940735\n",
      "[Epoch 1061/2000] Train Loss: 560.602325 | Val Loss: 566.613525\n",
      "[Epoch 1062/2000] Train Loss: 561.557846 | Val Loss: 487.211090\n",
      "[Epoch 1063/2000] Train Loss: 558.877869 | Val Loss: 474.575775\n",
      "[Epoch 1064/2000] Train Loss: 557.763084 | Val Loss: 556.620300\n",
      "[Epoch 1065/2000] Train Loss: 554.967171 | Val Loss: 558.980225\n",
      "[Epoch 1066/2000] Train Loss: 552.964767 | Val Loss: 524.535828\n",
      "[Epoch 1067/2000] Train Loss: 553.239216 | Val Loss: 542.958618\n",
      "[Epoch 1068/2000] Train Loss: 550.340466 | Val Loss: 530.133972\n",
      "[Epoch 1069/2000] Train Loss: 548.438164 | Val Loss: 499.105347\n",
      "[Epoch 1070/2000] Train Loss: 547.905006 | Val Loss: 501.418701\n",
      "[Epoch 1071/2000] Train Loss: 546.797623 | Val Loss: 552.116455\n",
      "[Epoch 1072/2000] Train Loss: 547.520798 | Val Loss: 507.397980\n",
      "[Epoch 1073/2000] Train Loss: 546.721184 | Val Loss: 477.479431\n",
      "[Epoch 1074/2000] Train Loss: 542.942596 | Val Loss: 532.213440\n",
      "[Epoch 1075/2000] Train Loss: 540.048168 | Val Loss: 536.526184\n",
      "[Epoch 1076/2000] Train Loss: 540.006622 | Val Loss: 571.316162\n",
      "[Epoch 1077/2000] Train Loss: 537.273170 | Val Loss: 496.018219\n",
      "[Epoch 1078/2000] Train Loss: 536.711250 | Val Loss: 494.600403\n",
      "[Epoch 1079/2000] Train Loss: 535.708988 | Val Loss: 489.523560\n",
      "[Epoch 1080/2000] Train Loss: 534.367634 | Val Loss: 490.782745\n",
      "[Epoch 1081/2000] Train Loss: 532.481117 | Val Loss: 497.817749\n",
      "[Epoch 1082/2000] Train Loss: 531.002815 | Val Loss: 542.048218\n",
      "[Epoch 1083/2000] Train Loss: 530.390530 | Val Loss: 534.204773\n",
      "[Epoch 1084/2000] Train Loss: 527.280575 | Val Loss: 467.813354\n",
      "[Epoch 1085/2000] Train Loss: 527.918560 | Val Loss: 480.465637\n",
      "[Epoch 1086/2000] Train Loss: 524.502029 | Val Loss: 517.713135\n",
      "[Epoch 1087/2000] Train Loss: 524.415878 | Val Loss: 517.030457\n",
      "[Epoch 1088/2000] Train Loss: 523.541481 | Val Loss: 507.268829\n",
      "[Epoch 1089/2000] Train Loss: 522.011143 | Val Loss: 517.793640\n",
      "[Epoch 1090/2000] Train Loss: 520.365883 | Val Loss: 500.388336\n",
      "[Epoch 1091/2000] Train Loss: 519.131123 | Val Loss: 449.729248\n",
      "[Epoch 1092/2000] Train Loss: 516.933792 | Val Loss: 439.077942\n",
      "[Epoch 1093/2000] Train Loss: 516.245182 | Val Loss: 469.378845\n",
      "[Epoch 1094/2000] Train Loss: 514.221973 | Val Loss: 501.371124\n",
      "[Epoch 1095/2000] Train Loss: 513.510956 | Val Loss: 486.187042\n",
      "[Epoch 1096/2000] Train Loss: 512.069798 | Val Loss: 497.139587\n",
      "[Epoch 1097/2000] Train Loss: 509.870506 | Val Loss: 510.688965\n",
      "[Epoch 1098/2000] Train Loss: 508.576397 | Val Loss: 488.101715\n",
      "[Epoch 1099/2000] Train Loss: 509.107716 | Val Loss: 489.351440\n",
      "[Epoch 1100/2000] Train Loss: 507.690125 | Val Loss: 461.952454\n",
      "[Epoch 1101/2000] Train Loss: 506.926815 | Val Loss: 457.994019\n",
      "[Epoch 1102/2000] Train Loss: 503.653378 | Val Loss: 493.417908\n",
      "[Epoch 1103/2000] Train Loss: 502.464638 | Val Loss: 533.029663\n",
      "[Epoch 1104/2000] Train Loss: 502.158062 | Val Loss: 489.306946\n",
      "[Epoch 1105/2000] Train Loss: 501.787338 | Val Loss: 438.946808\n",
      "[Epoch 1106/2000] Train Loss: 498.076748 | Val Loss: 490.836273\n",
      "[Epoch 1107/2000] Train Loss: 497.955967 | Val Loss: 499.376068\n",
      "[Epoch 1108/2000] Train Loss: 495.743832 | Val Loss: 496.788452\n",
      "[Epoch 1109/2000] Train Loss: 495.187904 | Val Loss: 447.744659\n",
      "[Epoch 1110/2000] Train Loss: 492.631908 | Val Loss: 474.928864\n",
      "[Epoch 1111/2000] Train Loss: 491.520515 | Val Loss: 479.012634\n",
      "[Epoch 1112/2000] Train Loss: 489.949192 | Val Loss: 445.027649\n",
      "[Epoch 1113/2000] Train Loss: 489.973660 | Val Loss: 479.366547\n",
      "[Epoch 1114/2000] Train Loss: 488.637169 | Val Loss: 417.002838\n",
      "[Epoch 1115/2000] Train Loss: 486.596252 | Val Loss: 425.629608\n",
      "[Epoch 1116/2000] Train Loss: 485.121098 | Val Loss: 486.791168\n",
      "[Epoch 1117/2000] Train Loss: 485.440052 | Val Loss: 487.555695\n",
      "[Epoch 1118/2000] Train Loss: 483.525326 | Val Loss: 475.269470\n",
      "[Epoch 1119/2000] Train Loss: 480.057045 | Val Loss: 445.669739\n",
      "[Epoch 1120/2000] Train Loss: 478.733509 | Val Loss: 386.454529\n",
      "[Epoch 1121/2000] Train Loss: 479.885094 | Val Loss: 410.065521\n",
      "[Epoch 1122/2000] Train Loss: 475.338581 | Val Loss: 484.651306\n",
      "[Epoch 1123/2000] Train Loss: 475.850304 | Val Loss: 484.850281\n",
      "[Epoch 1124/2000] Train Loss: 473.522160 | Val Loss: 485.182861\n",
      "[Epoch 1125/2000] Train Loss: 473.315273 | Val Loss: 447.675964\n",
      "[Epoch 1126/2000] Train Loss: 473.339428 | Val Loss: 402.552277\n",
      "[Epoch 1127/2000] Train Loss: 470.351009 | Val Loss: 413.082489\n",
      "[Epoch 1128/2000] Train Loss: 470.859531 | Val Loss: 451.331116\n",
      "[Epoch 1129/2000] Train Loss: 469.078781 | Val Loss: 460.471069\n",
      "[Epoch 1130/2000] Train Loss: 467.680561 | Val Loss: 441.385498\n",
      "[Epoch 1131/2000] Train Loss: 465.291576 | Val Loss: 439.513824\n",
      "[Epoch 1132/2000] Train Loss: 462.450378 | Val Loss: 440.498230\n",
      "[Epoch 1133/2000] Train Loss: 462.799316 | Val Loss: 454.299896\n",
      "[Epoch 1134/2000] Train Loss: 462.046051 | Val Loss: 466.314117\n",
      "[Epoch 1135/2000] Train Loss: 462.617870 | Val Loss: 448.579651\n",
      "[Epoch 1136/2000] Train Loss: 459.572361 | Val Loss: 436.294220\n",
      "[Epoch 1137/2000] Train Loss: 458.264641 | Val Loss: 438.135864\n",
      "[Epoch 1138/2000] Train Loss: 458.126293 | Val Loss: 386.456451\n",
      "[Epoch 1139/2000] Train Loss: 455.824265 | Val Loss: 388.823608\n",
      "[Epoch 1140/2000] Train Loss: 453.618435 | Val Loss: 423.171906\n",
      "[Epoch 1141/2000] Train Loss: 451.196217 | Val Loss: 471.716156\n",
      "[Epoch 1142/2000] Train Loss: 451.553986 | Val Loss: 511.977692\n",
      "[Epoch 1143/2000] Train Loss: 450.349205 | Val Loss: 416.128571\n",
      "[Epoch 1144/2000] Train Loss: 449.220459 | Val Loss: 378.385284\n",
      "[Epoch 1145/2000] Train Loss: 447.545731 | Val Loss: 425.910736\n",
      "[Epoch 1146/2000] Train Loss: 445.544807 | Val Loss: 482.548004\n",
      "[Epoch 1147/2000] Train Loss: 444.642822 | Val Loss: 407.552338\n",
      "[Epoch 1148/2000] Train Loss: 444.591293 | Val Loss: 382.669312\n",
      "[Epoch 1149/2000] Train Loss: 444.031094 | Val Loss: 427.355408\n",
      "[Epoch 1150/2000] Train Loss: 441.001339 | Val Loss: 408.015015\n",
      "[Epoch 1151/2000] Train Loss: 441.141243 | Val Loss: 395.110046\n",
      "[Epoch 1152/2000] Train Loss: 438.966072 | Val Loss: 353.774872\n",
      "[Epoch 1153/2000] Train Loss: 437.427597 | Val Loss: 375.361847\n",
      "[Epoch 1154/2000] Train Loss: 436.592449 | Val Loss: 409.764465\n",
      "[Epoch 1155/2000] Train Loss: 437.049107 | Val Loss: 440.972595\n",
      "[Epoch 1156/2000] Train Loss: 434.507374 | Val Loss: 447.958313\n",
      "[Epoch 1157/2000] Train Loss: 433.088821 | Val Loss: 449.175171\n",
      "[Epoch 1158/2000] Train Loss: 429.828209 | Val Loss: 417.107117\n",
      "[Epoch 1159/2000] Train Loss: 430.806877 | Val Loss: 401.405914\n",
      "[Epoch 1160/2000] Train Loss: 429.079395 | Val Loss: 365.156891\n",
      "[Epoch 1161/2000] Train Loss: 428.470509 | Val Loss: 385.432739\n",
      "[Epoch 1162/2000] Train Loss: 427.030941 | Val Loss: 422.732239\n",
      "[Epoch 1163/2000] Train Loss: 425.020561 | Val Loss: 429.076874\n",
      "[Epoch 1164/2000] Train Loss: 424.010098 | Val Loss: 392.398590\n",
      "[Epoch 1165/2000] Train Loss: 422.639828 | Val Loss: 369.096344\n",
      "[Epoch 1166/2000] Train Loss: 421.371906 | Val Loss: 388.191132\n",
      "[Epoch 1167/2000] Train Loss: 420.010502 | Val Loss: 408.484924\n",
      "[Epoch 1168/2000] Train Loss: 419.896790 | Val Loss: 423.868774\n",
      "[Epoch 1169/2000] Train Loss: 419.063148 | Val Loss: 453.511047\n",
      "[Epoch 1170/2000] Train Loss: 417.711643 | Val Loss: 409.312805\n",
      "[Epoch 1171/2000] Train Loss: 417.456474 | Val Loss: 375.617279\n",
      "[Epoch 1172/2000] Train Loss: 413.793667 | Val Loss: 377.246460\n",
      "[Epoch 1173/2000] Train Loss: 413.326168 | Val Loss: 377.590698\n",
      "[Epoch 1174/2000] Train Loss: 412.041668 | Val Loss: 394.580658\n",
      "[Epoch 1175/2000] Train Loss: 410.354374 | Val Loss: 379.238342\n",
      "[Epoch 1176/2000] Train Loss: 408.954475 | Val Loss: 397.353302\n",
      "[Epoch 1177/2000] Train Loss: 407.640186 | Val Loss: 378.476593\n",
      "[Epoch 1178/2000] Train Loss: 407.635548 | Val Loss: 375.305786\n",
      "[Epoch 1179/2000] Train Loss: 406.126358 | Val Loss: 384.411438\n",
      "[Epoch 1180/2000] Train Loss: 403.901684 | Val Loss: 340.650879\n",
      "[Epoch 1181/2000] Train Loss: 404.254967 | Val Loss: 357.306244\n",
      "[Epoch 1182/2000] Train Loss: 402.025814 | Val Loss: 380.200958\n",
      "[Epoch 1183/2000] Train Loss: 401.409313 | Val Loss: 360.229034\n",
      "[Epoch 1184/2000] Train Loss: 399.670906 | Val Loss: 361.329620\n",
      "[Epoch 1185/2000] Train Loss: 397.711040 | Val Loss: 357.972046\n",
      "[Epoch 1186/2000] Train Loss: 397.263939 | Val Loss: 375.511597\n",
      "[Epoch 1187/2000] Train Loss: 394.785446 | Val Loss: 376.525024\n",
      "[Epoch 1188/2000] Train Loss: 395.740299 | Val Loss: 379.747437\n",
      "[Epoch 1189/2000] Train Loss: 393.753696 | Val Loss: 409.774139\n",
      "[Epoch 1190/2000] Train Loss: 393.741596 | Val Loss: 403.457886\n",
      "[Epoch 1191/2000] Train Loss: 390.392948 | Val Loss: 360.769714\n",
      "[Epoch 1192/2000] Train Loss: 389.840782 | Val Loss: 330.433807\n",
      "[Epoch 1193/2000] Train Loss: 388.728268 | Val Loss: 356.219055\n",
      "[Epoch 1194/2000] Train Loss: 386.913628 | Val Loss: 378.510681\n",
      "[Epoch 1195/2000] Train Loss: 386.854168 | Val Loss: 367.430817\n",
      "[Epoch 1196/2000] Train Loss: 386.558998 | Val Loss: 353.677521\n",
      "[Epoch 1197/2000] Train Loss: 386.471268 | Val Loss: 367.928192\n",
      "[Epoch 1198/2000] Train Loss: 382.190563 | Val Loss: 361.327698\n",
      "[Epoch 1199/2000] Train Loss: 382.503822 | Val Loss: 371.372742\n",
      "[Epoch 1200/2000] Train Loss: 382.363270 | Val Loss: 392.373138\n",
      "[Epoch 1201/2000] Train Loss: 380.620876 | Val Loss: 377.442596\n",
      "[Epoch 1202/2000] Train Loss: 377.881840 | Val Loss: 373.123810\n",
      "[Epoch 1203/2000] Train Loss: 377.790371 | Val Loss: 355.386749\n",
      "[Epoch 1204/2000] Train Loss: 376.352531 | Val Loss: 332.987396\n",
      "[Epoch 1205/2000] Train Loss: 372.685307 | Val Loss: 340.401367\n",
      "[Epoch 1206/2000] Train Loss: 375.779850 | Val Loss: 352.833160\n",
      "[Epoch 1207/2000] Train Loss: 372.860184 | Val Loss: 338.750122\n",
      "[Epoch 1208/2000] Train Loss: 373.009716 | Val Loss: 358.969818\n",
      "[Epoch 1209/2000] Train Loss: 370.391605 | Val Loss: 382.325562\n",
      "[Epoch 1210/2000] Train Loss: 369.153542 | Val Loss: 337.212616\n",
      "[Epoch 1211/2000] Train Loss: 368.211002 | Val Loss: 322.094818\n",
      "[Epoch 1212/2000] Train Loss: 366.578007 | Val Loss: 287.701050\n",
      "[Epoch 1213/2000] Train Loss: 366.360584 | Val Loss: 306.866913\n",
      "[Epoch 1214/2000] Train Loss: 366.494186 | Val Loss: 342.539856\n",
      "[Epoch 1215/2000] Train Loss: 363.921238 | Val Loss: 386.621613\n",
      "[Epoch 1216/2000] Train Loss: 363.345013 | Val Loss: 341.994324\n",
      "[Epoch 1217/2000] Train Loss: 360.745193 | Val Loss: 316.433655\n",
      "[Epoch 1218/2000] Train Loss: 360.052826 | Val Loss: 341.187164\n",
      "[Epoch 1219/2000] Train Loss: 360.558510 | Val Loss: 351.820953\n",
      "[Epoch 1220/2000] Train Loss: 358.935230 | Val Loss: 367.607727\n",
      "[Epoch 1221/2000] Train Loss: 358.217728 | Val Loss: 343.960815\n",
      "[Epoch 1222/2000] Train Loss: 356.285210 | Val Loss: 274.332397\n",
      "[Epoch 1223/2000] Train Loss: 354.389816 | Val Loss: 285.102783\n",
      "[Epoch 1224/2000] Train Loss: 355.145058 | Val Loss: 367.459290\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     27\u001b[39m     model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvali_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_out\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_in\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_out\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1414\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\multiprocessing\\queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\multiprocessing\\connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\multiprocessing\\connection.py:346\u001b[39m, in \u001b[36mPipeConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    344\u001b[39m             _winapi.PeekNamedPipe(\u001b[38;5;28mself\u001b[39m._handle)[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\multiprocessing\\connection.py:1084\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1081\u001b[39m                 ready_objects.add(o)\n\u001b[32m   1082\u001b[39m                 timeout = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m     ready_handles = \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1086\u001b[39m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gusdb\\anaconda3\\envs\\medsam2\\Lib\\multiprocessing\\connection.py:1016\u001b[39m, in \u001b[36m_exhaustive_wait\u001b[39m\u001b[34m(handles, timeout)\u001b[39m\n\u001b[32m   1014\u001b[39m ready = []\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m     res = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == WAIT_TIMEOUT:\n\u001b[32m   1018\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "plt.ion()  # interactive mode ON\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss_train = 0.0\n",
    "    epoch_loss_vali = 0.0\n",
    "\n",
    "    for batch_in, batch_out in train_dataloader:\n",
    "        batch_in, batch_out = batch_in.to(device), batch_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_in)\n",
    "        loss = criterion(y_pred, batch_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "\n",
    "    avg_train_loss = epoch_loss_train / len(train_dataloader)\n",
    "            \n",
    "    train_loss_list.append(avg_train_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in vali_dataloader:\n",
    "            batch_in, batch_out = batch_in.to(device), batch_out.to(device)\n",
    "            y_pred = model(batch_in)\n",
    "            val_loss = criterion(y_pred, batch_out)\n",
    "\n",
    "            epoch_loss_vali += val_loss.item()\n",
    "\n",
    "\n",
    "    avg_vali_loss = epoch_loss_vali / len(vali_dataloader)\n",
    "    vali_loss_list.append(avg_vali_loss)\n",
    "\n",
    "    # --- 실시간 그래프 업데이트 ---\n",
    "    ax.clear()\n",
    "    epochs_ran = range(len(train_loss_list))\n",
    "    ax.plot(epochs_ran, train_loss_list, label=\"Training Loss\", color=\"blue\")\n",
    "    ax.plot(epochs_ran, vali_loss_list, label=\"Validation Loss\", color=\"red\") # validation -> Validation (오타 수정)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE Loss\")\n",
    "    ax.set_title(f\"Simple_MLP Training Loss [Epoch {epoch+1}/{epochs}]\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    plt.pause(0.01) # <--- [수정 3: 그래프가 확실히 보이도록 추가]\n",
    "\n",
    "    # 훈련 손실과 검증 손실을 함께 출력하는 것이 좋습니다.\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_vali_loss:.6f}\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# ----- 모델 저장 -----\n",
    "torch.save(model.state_dict(), \"Simple_MLP.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
